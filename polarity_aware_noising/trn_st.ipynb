{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb40e83-94f3-4203-b978-0c70ccdd5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "import SentimentTransfer_Evaluations as snt_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a984f-4a7c-44c7-bacd-b2219cb4a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = open(\"config.json\")\n",
    "config_obj = json.load(config_file)\n",
    "\n",
    "json_dumps = json.dumps(config_obj)\n",
    "print(json_dumps, flush=True)\n",
    "\n",
    "dir = config_obj[\"dir\"]\n",
    "field_fix_length = config_obj[\"field_fix_length\"]\n",
    "\n",
    "train_pos_data_path = config_obj[\"train_pos_data_path\"]\n",
    "train_neg_data_path = config_obj[\"train_neg_data_path\"]\n",
    "valid_pos_data_path = config_obj[\"valid_pos_data_path\"]\n",
    "valid_neg_data_path = config_obj[\"valid_neg_data_path\"]\n",
    "test_pos_data_path = config_obj[\"test_pos_data_path\"]\n",
    "test_neg_data_path = config_obj[\"test_neg_data_path\"]\n",
    "\n",
    "src_ext = config_obj[\"src_ext\"]\n",
    "trg_ext = config_obj[\"trg_ext\"]\n",
    "vocab_min_freq = config_obj[\"vocab_min_freq\"]\n",
    "vocab_max_size = config_obj[\"vocab_max_size\"]\n",
    "batch_size = config_obj[\"batch_size\"]\n",
    "encoder_max_length = config_obj[\"encoder_max_length\"]\n",
    "decoder_max_length = config_obj[\"decoder_max_length\"]\n",
    "style_num_embeddings = config_obj[\"style_num_embeddings\"]\n",
    "style_embedding_dim = config_obj[\"style_embedding_dim\"]\n",
    "hid_dim_enc = config_obj[\"hid_dim_enc\"]\n",
    "hid_dim_dec = config_obj[\"hid_dim_dec\"]\n",
    "enc_layers = config_obj[\"enc_layers\"]\n",
    "dec_layers = config_obj[\"dec_layers\"]\n",
    "enc_heads = config_obj[\"enc_heads\"]\n",
    "dec_heads = config_obj[\"dec_heads\"]\n",
    "enc_pf_dim = config_obj[\"enc_pf_dim\"]\n",
    "dec_pf_dim = config_obj[\"dec_pf_dim\"]\n",
    "enc_dropout = config_obj[\"enc_dropout\"]\n",
    "dec_dropout = config_obj[\"dec_dropout\"]\n",
    "learning_rate = config_obj[\"learning_rate\"]\n",
    "add_loss_t_s_start_epoch = config_obj[\"add_loss_t_s_start_epoch\"]\n",
    "add_loss_t_s_end_epoch = config_obj[\"add_loss_t_s_end_epoch\"]\n",
    "add_loss_t_w_start_epoch = config_obj[\"add_loss_t_w_start_epoch\"]\n",
    "add_loss_t_w_end_epoch = config_obj[\"add_loss_t_w_end_epoch\"]\n",
    "add_loss_s_w_start_epoch = config_obj[\"add_loss_s_w_start_epoch\"]\n",
    "add_loss_s_w_end_epoch = config_obj[\"add_loss_s_w_end_epoch\"]\n",
    "add_loss_t_s_w_start_epoch = config_obj[\"add_loss_t_s_w_start_epoch\"]\n",
    "add_loss_t_s_w_end_epoch = config_obj[\"add_loss_t_s_w_end_epoch\"]\n",
    "alt_loss_t_s_start_epoch = config_obj[\"alt_loss_t_s_start_epoch\"]\n",
    "alt_loss_t_s_end_epoch = config_obj[\"alt_loss_t_s_end_epoch\"]\n",
    "alt_loss_t_w_start_epoch = config_obj[\"alt_loss_t_w_start_epoch\"]\n",
    "alt_loss_t_w_end_epoch = config_obj[\"alt_loss_t_w_end_epoch\"]\n",
    "alt_loss_s_w_start_epoch = config_obj[\"alt_loss_s_w_start_epoch\"]\n",
    "alt_loss_s_w_end_epoch = config_obj[\"alt_loss_s_w_end_epoch\"]\n",
    "alt_loss_t_s_w_start_epoch = config_obj[\"alt_loss_t_s_w_start_epoch\"]\n",
    "alt_loss_t_s_w_end_epoch = config_obj[\"alt_loss_t_s_w_end_epoch\"]\n",
    "translation_loss_start_epoch = config_obj[\"translation_loss_start_epoch\"]\n",
    "translation_loss_end_epoch = config_obj[\"translation_loss_end_epoch\"]\n",
    "style_loss_start_epoch = config_obj[\"style_loss_start_epoch\"]\n",
    "style_loss_end_epoch = config_obj[\"style_loss_end_epoch\"]\n",
    "words_style_loss_start_epoch = config_obj[\"words_style_loss_start_epoch\"]\n",
    "words_style_loss_end_epoch = config_obj[\"words_style_loss_end_epoch\"]\n",
    "\n",
    "num_epochs = config_obj[\"num_epochs\"]\n",
    "clip = config_obj[\"clip\"]\n",
    "early_stop_lookout = config_obj[\"early_stop_lookout\"]\n",
    "another_early_stop_lookout = config_obj[\"another_early_stop_lookout\"]\n",
    "add_loss_t_s = config_obj[\"add_loss_t_s\"]\n",
    "add_loss_t_w = config_obj[\"add_loss_t_w\"]\n",
    "add_loss_s_w = config_obj[\"add_loss_s_w\"]\n",
    "add_loss_t_s_w = config_obj[\"add_loss_t_s_w\"]\n",
    "alt_loss_t_s = config_obj[\"alt_loss_t_s\"]\n",
    "alt_loss_t_w = config_obj[\"alt_loss_t_w\"]\n",
    "alt_loss_s_w = config_obj[\"alt_loss_s_w\"]\n",
    "alt_loss_t_s_w = config_obj[\"alt_loss_t_s_w\"]\n",
    "t_loss = config_obj[\"translation_loss\"]\n",
    "s_s_loss = config_obj[\"style_loss\"]\n",
    "w_s_loss = config_obj[\"words_style_loss\"]\n",
    "v_p_loss = config_obj[\"vocab_prob_loss\"]\n",
    "v_p_loss_start_epoch = config_obj[\"vocab_prob_loss_start_epoch\"]\n",
    "v_p_loss_end_epoch = config_obj[\"vocab_prob_loss_end_epoch\"]\n",
    "add_loss_t_v = config_obj[\"add_loss_t_v\"]\n",
    "add_loss_t_v_start_epoch = config_obj[\"add_loss_t_v_start_epoch\"]\n",
    "add_loss_t_v_end_epoch = config_obj[\"add_loss_t_v_end_epoch\"]\n",
    "alt_loss_t_v = config_obj[\"alt_loss_t_v\"]\n",
    "alt_loss_t_v_start_epoch = config_obj[\"alt_loss_t_v_start_epoch\"]\n",
    "alt_loss_t_v_end_epoch = config_obj[\"alt_loss_t_v_end_epoch\"]\n",
    "translation_loss_weight = config_obj[\"translation_loss_weight\"]\n",
    "vocab_loss_weight = config_obj[\"vocab_loss_weight\"]\n",
    "ss_loss_weight = config_obj[\"ss_loss_weight\"]\n",
    "ws_loss_weight = config_obj[\"ws_loss_weight\"]\n",
    "check_best_after_epoch = config_obj[\"check_best_after_epoch\"]\n",
    "check_best_after_epoch2 = config_obj[\"check_best_after_epoch2\"]\n",
    "specific_epoch_checkpoint = config_obj[\"specific_epoch_checkpoint\"]\n",
    "debug = config_obj[\"debug\"]\n",
    "style_cond = config_obj[\"style_cond\"]\n",
    "is_only_evaluation = config_obj[\"is_only_evaluation\"]\n",
    "pretrn_encoder = config_obj[\"pretrn_encoder\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b58d2-b7cb-4716-aee4-0f68e86841b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373bc12f-9d2e-41d7-b17e-6771e128e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_fr = spacy.load('de_core_news_sm')\n",
    "spacy_fr.tokenizer.add_special_case(u'<style>', [{ORTH: u'<style>'}])\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bc3f0a-77c1-4581-a074-e5c8f535cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_field(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return dill.load(f)\n",
    "\n",
    "#Please provide the path for the pretrained src field from the noisy translation\n",
    "SRC = load_field(os.path.join(\"\", 'src.field'))\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            # tokenize = 'spacy',\n",
    "            # tokenizer_language='de',\n",
    "            init_token='<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            batch_first = True,\n",
    "            fix_length=field_fix_length)\n",
    "\n",
    "train_pos_data = TranslationDataset(\n",
    "    path=train_neg_data_path+str(0),\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "\n",
    "TRG.build_vocab(train_pos_data, min_freq=vocab_min_freq, max_size=vocab_max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a35ee-96a4-4677-a3f0-67d0e91a4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in source vocabulary: {len(SRC.vocab)}\", flush=True)\n",
    "print(f\"Unique tokens in target vocabulary: {len(TRG.vocab)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d6aa5-275e-4df2-ad8c-7b48bb8c78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos_data = TranslationDataset(\n",
    "    path=test_neg_data_path+str(0),\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "test_neg_data = TranslationDataset(\n",
    "    path=test_pos_data_path+str(0),\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57769bb8-de82-4118-aea3-a17ff25a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "BATCH_SIZE = batch_size\n",
    "\n",
    "def data_read(epoch):\n",
    "    train_pos_data = TranslationDataset(\n",
    "        path=train_neg_data_path+str(epoch),\n",
    "        exts=(src_ext, trg_ext),\n",
    "        fields=(SRC, TRG),\n",
    "    )\n",
    "    # train_neg_data = TranslationDataset(\n",
    "    #     path=train_neg_data_path,\n",
    "    #     exts=(src_ext, trg_ext),\n",
    "    #     fields=(SRC, TRG),\n",
    "    # )\n",
    "    valid_pos_data = TranslationDataset(\n",
    "        path=valid_neg_data_path+str(epoch),\n",
    "        exts=(src_ext, trg_ext),\n",
    "        fields=(SRC, TRG),\n",
    "    )\n",
    "    # valid_neg_data = TranslationDataset(\n",
    "    #     path=valid_neg_data_path,\n",
    "    #     exts=(src_ext, trg_ext),\n",
    "    #     fields=(SRC, TRG),\n",
    "    # )\n",
    "    test_pos_data = TranslationDataset(\n",
    "        path=test_neg_data_path+str(epoch),\n",
    "        exts=(src_ext, trg_ext),\n",
    "        fields=(SRC, TRG),\n",
    "    )\n",
    "    # test_neg_data = TranslationDataset(\n",
    "    #     path=test_neg_data_path,\n",
    "    #     exts=(src_ext, trg_ext),\n",
    "    #     fields=(SRC, TRG),\n",
    "    # )\n",
    "    #######################################################################################################################\n",
    "    print(f\"Number of training examples: {len(train_pos_data.examples)}\", flush=True)\n",
    "    #print(f\"Number of training examples: {len(train_neg_data.examples)}\", flush=True)\n",
    "\n",
    "    print(f\"Number of validation examples: {len(valid_pos_data.examples)}\", flush=True)\n",
    "    #print(f\"Number of validation examples: {len(valid_neg_data.examples)}\", flush=True)\n",
    "\n",
    "    print(f\"Number of testing examples: {len(test_pos_data.examples)}\", flush=True)\n",
    "\n",
    "    # print(f\"Number of training examples: {len(train_data.examples)}\", flush=True)\n",
    "    # print(f\"Number of validation examples: {len(valid_data.examples)}\", flush=True)\n",
    "    # print(f\"Number of testing examples: {len(test_data.examples)}\", flush=True)\n",
    "    #######################################################################################################################\n",
    "    # train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    #     (train_data, valid_data, test_data),\n",
    "    train_pos_iterator, valid_pos_iterator, test_pos_iterator = BucketIterator.splits(\n",
    "        (train_pos_data, valid_pos_data, test_pos_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_within_batch=True,\n",
    "        sort_key= lambda x: len(x.src),\n",
    "        device = device)\n",
    "    return train_pos_iterator, valid_pos_iterator, test_pos_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2db84cb-2d5b-4601-83a2-4da15b0903f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=encoder_max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch size, src len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, src len]\n",
    "\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33bbf6-1ddd-4470-91f4-e1a44464741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch size, src len, hid dim]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f102e2-16f5-4dd2-8886-e052ce1b27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query = [batch size, query len, hid dim]\n",
    "        # key = [batch size, key len, hid dim]\n",
    "        # value = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q = [batch size, query len, hid dim]\n",
    "        # K = [batch size, key len, hid dim]\n",
    "        # V = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q = [batch size, n heads, query len, head dim]\n",
    "        # K = [batch size, n heads, key len, head dim]\n",
    "        # V = [batch size, n heads, value len, head dim]\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # energy = [batch size, n heads, query len, key len]\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention = [batch size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x = [batch size, n heads, query len, head dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x = [batch size, query len, n heads, head dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        return x, attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfd479-3a57-4bea-ae97-98abc34f7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x = [batch size, seq len, pf dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2844eab5-79d0-4a75-95b3-b2dc0c8c1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=decoder_max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, trg len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23b556a-f611-439f-8074-0b9b9a7cd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70415f6a-2fa0-483b-9e4c-c37816bd881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src = [batch size, src len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        # trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return output, attention\n",
    "\n",
    "class Seq2Seq2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrnd_model,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrnd_model = pretrnd_model\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src = [batch size, src len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        # trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        enc_src = self.pretrnd_model.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d54e4-c532-4560-a24a-ca990a7bf610",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "#HID_DIM = 512\n",
    "HID_DIM_ENC = hid_dim_enc\n",
    "HID_DIM_DEC = hid_dim_dec\n",
    "ENC_LAYERS = enc_layers\n",
    "DEC_LAYERS = dec_layers\n",
    "ENC_HEADS = enc_heads\n",
    "DEC_HEADS = dec_heads\n",
    "ENC_PF_DIM = enc_pf_dim\n",
    "DEC_PF_DIM = dec_pf_dim\n",
    "ENC_DROPOUT = enc_dropout\n",
    "DEC_DROPOUT = dec_dropout\n",
    "\n",
    "enc = Encoder(INPUT_DIM,\n",
    "              HID_DIM_ENC,\n",
    "              ENC_LAYERS,\n",
    "              ENC_HEADS,\n",
    "              ENC_PF_DIM,\n",
    "              ENC_DROPOUT,\n",
    "              device)\n",
    "dec_pretrained = Decoder(15505,\n",
    "              HID_DIM_DEC,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM,\n",
    "              HID_DIM_DEC,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed7b99-60d1-4411-a6a0-a83c86d9e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "pretrnd_model = Seq2Seq(enc, dec_pretrained, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "wmt_checkpoint = torch.load(\"\")\n",
    "pretrnd_model.load_state_dict(wmt_checkpoint['state_dict'])\n",
    "model = Seq2Seq2(pretrnd_model, enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcdc4e-a39a-4969-b637-df7312cacd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12813722-07b4-4d1b-8b9f-d3c7256e838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initialize_weights(m):\n",
    "#     if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(m.weight.data)\n",
    "# if is_only_evaluation == False:\n",
    "#     model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd3413-cc97-48b6-807e-27a5fe58d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_dict = torch.load(\"../../simple_translation/wmt_translation\"+'/checkpoint.pt')['state_dict']\n",
    "# # with open('dict_keys', 'w')as op:\n",
    "# # for k, v in torch.load(\"../../simple_translation/wmt_translation\"+'/checkpoint.pt')['state_dict'].items():\n",
    "# #     op.write(k)\n",
    "# #     op.write('\\n')\n",
    "# # pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model.state_dict()}\n",
    "# enc_dict = {}\n",
    "# enc_keys = ['encoder.'+k for k in model.encoder.state_dict().keys()]\n",
    "# for k, v in pretrained_dict.items():\n",
    "#     if k in enc_keys:\n",
    "#         enc_dict[k.replace('encoder.', '')]=v\n",
    "# model.encoder.state_dict().update(enc_dict)\n",
    "# model.encoder.load_state_dict(enc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab80b68-25f7-4e19-871b-5af68d9054c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0008f25-6e9c-4e6c-a72e-044bee2ee48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8316ee55-80c1-4ba0-8813-97b755e17e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(dir+'/checkpoint_7.pt')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# save_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42f07f-2f29-4ae4-b894-8f20d1c25342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_translation(criterion, output, trg):\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.contiguous().view(-1, output_dim)\n",
    "    trg = trg[:, 1:].contiguous().view(-1)\n",
    "    # output = [batch size * trg len - 1, output dim]\n",
    "    # trg = [batch size * trg len - 1]\n",
    "    translation_loss = criterion(output, trg)\n",
    "    return translation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce76d7e-f289-4cc2-9f85-3d086058715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list_enc = [3 for i in range(100)]\n",
    "neg_list_enc = [4 for i in range(100)]\n",
    "\n",
    "pos_list_dec = [3 for i in range(99)]\n",
    "neg_list_dec = [4 for i in range(99)]\n",
    "\n",
    "def calculate_output_loss(epoch, model, pos_iterator, criterion, is_eval, optimizer=None, clip=None):\n",
    "    epoch_translation_loss = 0\n",
    "\n",
    "    total_no_batches = 0\n",
    "\n",
    "    for i, batch in enumerate(pos_iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        if is_eval==False:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(src, trg[:, :-1])\n",
    "\n",
    "        translation_loss = loss_translation(criterion, output, trg)\n",
    "\n",
    "        if is_eval == False:\n",
    "            translation_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_translation_loss += translation_loss.item()\n",
    "        total_no_batches +=1\n",
    "\n",
    "    epoch_translation_loss_avg = epoch_translation_loss / total_no_batches\n",
    "\n",
    "    return epoch_translation_loss_avg\n",
    "\n",
    "def train(epoch, model, pos_iterator, criterion, is_eval, optimizer=None, clip=None):\n",
    "    if is_eval == False:\n",
    "        model.train()\n",
    "        epoch_translation_loss = calculate_output_loss(epoch, model, pos_iterator, criterion, False, optimizer, clip)\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_translation_loss = calculate_output_loss(epoch, model, pos_iterator, criterion, True)\n",
    "    return epoch_translation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16889cba-3c94-46bc-952c-fd2fefc9d3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def post_processing(text_list):\n",
    "    repl_list = {'@@ ': '', '<eos>':''}\n",
    "    text_str = ' '.join(text_list)\n",
    "    return functools.reduce(lambda a, kv: a.replace(*kv), repl_list.items(), text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784292a-5eac-449b-81bc-58ead947591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668c90c-9251-4877-b5e5-939a074b90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_only_evaluation == False:\n",
    "    N_EPOCHS = num_epochs\n",
    "    CLIP = clip\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    valid_loss_before = float('inf')\n",
    "\n",
    "    early_stop_cnt = 0\n",
    "    early_stop_lookout = early_stop_lookout\n",
    "    early_stop=False\n",
    "\n",
    "    another_early_stop_cnt = 0\n",
    "    another_early_stop_lookout = another_early_stop_lookout\n",
    "    another_early_stop=False\n",
    "\n",
    "    best_epoch_no = 0\n",
    "\n",
    "    #for epoch in range(save_epoch, N_EPOCHS):\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_pos_iterator, valid_pos_iterator, test_pos_iterator = data_read(epoch)\n",
    "        train_translation_loss = train(epoch, model, train_pos_iterator, criterion, False, optimizer, CLIP)\n",
    "        valid_translation_loss = train(epoch, model, valid_pos_iterator, criterion, True)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, dir + f'/latest_checkpoint.pt')\n",
    "        if(epoch == specific_epoch_checkpoint):\n",
    "            torch.save(checkpoint, dir + f'/checkpoint_{specific_epoch_checkpoint}.pt')\n",
    "\n",
    "        if(epoch > check_best_after_epoch):\n",
    "            #Early_Stop\n",
    "            if valid_translation_loss < best_valid_loss or valid_translation_loss < valid_loss_before:\n",
    "                early_stop_cnt = 0\n",
    "                early_stop = False\n",
    "\n",
    "            elif valid_translation_loss >= best_valid_loss or valid_translation_loss >= valid_loss_before:\n",
    "                early_stop_cnt += 1\n",
    "                early_stop = True\n",
    "\n",
    "            # Another Early_Stop based on only best valid translation_loss\n",
    "            if epoch > check_best_after_epoch2 :\n",
    "                if valid_translation_loss < best_valid_loss:\n",
    "                    another_early_stop_cnt = 0\n",
    "                    another_early_stop = False\n",
    "\n",
    "                elif valid_translation_loss >= best_valid_loss:\n",
    "                    another_early_stop_cnt += 1\n",
    "                    another_early_stop = True\n",
    "\n",
    "\n",
    "            if valid_translation_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_translation_loss\n",
    "                best_epoch_no = epoch\n",
    "\n",
    "                torch.save(checkpoint, dir + '/best_valid_checkpoint.pt')\n",
    "                # if(epoch==4):\n",
    "                #     torch.save(checkpoint, dir+'/epoch5_checkpoint.pt')\n",
    "\n",
    "            valid_loss_before = valid_translation_loss\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s', flush=True)\n",
    "        print(f'\\tTrain Translation Loss: {train_translation_loss:.7f}', flush=True)\n",
    "        print(f'\\t Val. Translation Loss: {valid_translation_loss:.3f}', flush=True)\n",
    "        print(f'\\t Till now Best Val. Loss: {best_valid_loss:.3f} found on {best_epoch_no+1} epoch ', flush=True)\n",
    "\n",
    "        if early_stop==True :\n",
    "            print(f'EarlyStopping counter (1st way): {early_stop_cnt} out of {early_stop_lookout}', flush=True)\n",
    "\n",
    "        if another_early_stop==True :\n",
    "            print(f'EarlyStopping counter (2nd way): {another_early_stop_cnt} out of {another_early_stop_lookout}', flush=True)\n",
    "\n",
    "        print('\\n', flush=True)\n",
    "\n",
    "\n",
    "        if early_stop_cnt == early_stop_lookout:\n",
    "            print('Early Stoping (1st way)...', flush=True)\n",
    "            break\n",
    "        if another_early_stop_cnt == another_early_stop_lookout:\n",
    "            print('Early Stoping (2nd way)...', flush=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e442cd6-4a68-4226-b4ea-94ef706d4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = torch.load(dir+'/latest_checkpoint.pt')\n",
    "best_valid_checkpoint = torch.load(dir+'/best_valid_checkpoint.pt')\n",
    "\n",
    "model.load_state_dict(best_valid_checkpoint['state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#epoch = checkpoint['epoch']\n",
    "\n",
    "if is_only_evaluation == False:\n",
    "    test_loss = train(None, model, test_pos_iterator, criterion, True)\n",
    "    print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08e3f7b-a265-4004-b3ed-64fcc746914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "#     assert n_rows * n_cols == n_heads\n",
    "#\n",
    "#     fig = plt.figure(figsize=(15, 25))\n",
    "#\n",
    "#     for i in range(n_heads):\n",
    "#         ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "#\n",
    "#         _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "#\n",
    "#         cax = ax.matshow(_attention, cmap='bone')\n",
    "#\n",
    "#         ax.tick_params(labelsize=12)\n",
    "#         ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'],\n",
    "#                            rotation=45)\n",
    "#         ax.set_yticklabels([''] + translation)\n",
    "#\n",
    "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#         ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15d51fd-5a85-4e24-b367-fec09a400540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len, decoder_flag, is_st):\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    if(is_st == True):\n",
    "        if(decoder_flag=='pos'):\n",
    "            decoder_flag = 'neg'\n",
    "        elif(decoder_flag=='neg'):\n",
    "            decoder_flag = 'pos'\n",
    "\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[tokens[0]]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # if(decoder_flag == 'pos'):\n",
    "            #     output, attention = model.pos_decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            # elif(decoder_flag == 'neg'):\n",
    "            #     output, attention = model.neg_decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af0f00-255e-4df4-a166-f19132858e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f68eb5b-70be-4136-85b9-d383f5d136c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "import nltk\n",
    "senti_trg_list_ops = []\n",
    "def masked_sent(sent_list):\n",
    "    masked = None #snt_ev.mask_polarity(post_processing(sent_list))\n",
    "    return nltk.word_tokenize(masked)\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len, decoder_flag, is_st):\n",
    "    trgs = []\n",
    "    masked_trgs = []\n",
    "\n",
    "    pred_trgs = []\n",
    "    masked_pred_trgs = []\n",
    "\n",
    "    lengthy_idx = []\n",
    "    for idx, datum in enumerate(data):\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        if len(src) < 100 and len(trg) < 100:\n",
    "            pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, decoder_flag, is_st)\n",
    "\n",
    "            # cut off <eos> token\n",
    "            pred_trg = pred_trg[:-1]\n",
    "\n",
    "            pred_trgs.append(pred_trg)\n",
    "            #masked_pred_trgs.append(masked_sent(pred_trg))\n",
    "            trgs.append([trg])\n",
    "            #masked_trgs.append([masked_sent(trg)])\n",
    "        else:\n",
    "            lengthy_idx.append(idx)\n",
    "\n",
    "    # print(lengthy_idx)\n",
    "    # print(len(pred_trgs))\n",
    "    return bleu_score(pred_trgs, trgs), None #bleu_score(masked_pred_trgs, masked_trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1981b974-9018-410d-a113-9f894dcc3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_score, masked_b_score = calculate_bleu(test_pos_data, SRC, TRG, model, device, 50, 'pos', True)\n",
    "# print('\\n')\n",
    "# print(f'Style Transfer: BLEU score on test pos data = {b_score*100:.5f}', flush=True)\n",
    "# print(f'Style Transfer: Masked BLEU score on test pos data = {masked_b_score*100:.5f}', flush=True)\n",
    "b_score, masked_b_score = calculate_bleu(test_neg_data, SRC, TRG, model, device, 50, 'neg', True)\n",
    "print('\\n')\n",
    "print(f'Style Transfer: BLEU score on test neg data = {b_score*100:.5f}', flush=True)\n",
    "#print(f'Style Transfer: Masked BLEU score on test neg data = {masked_b_score*100:.5f}', flush=True)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef671746-002c-4d65-b1ed-e1c7bd315c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data, decoder_flag):\n",
    "    correct_count = 0\n",
    "    lm_scores = []\n",
    "    similarity_scores = []\n",
    "    masked_similarity_scores = []\n",
    "\n",
    "    for idx in range(1000):\n",
    "        example_idx = idx\n",
    "\n",
    "        src = vars(data.examples[example_idx])['src']\n",
    "        trg = vars(data.examples[example_idx])['trg']\n",
    "\n",
    "\n",
    "        #predicted_trg_trn, attention = translate_sentence(src, SRC, TRG, model, device, 50, decoder_flag, False)\n",
    "        predicted_trg_st, attention = translate_sentence(src, SRC, TRG, model, device, 50, decoder_flag, True)\n",
    "\n",
    "        print(f'src = {post_processing(src)}', flush=True)\n",
    "\n",
    "        print(f'trg = {post_processing(trg)}', flush=True)\n",
    "        result_trg = snt_ev.senti_score(post_processing(trg))\n",
    "        print(\"Label:\", result_trg['label'])\n",
    "        print(\"Confidence Score:\", result_trg['score'])\n",
    "\n",
    "\n",
    "        print(f'style transfered predicted trg = {post_processing(predicted_trg_st)}', flush=True)\n",
    "\n",
    "        # Sentiment Score\n",
    "        result_pred = snt_ev.senti_score(post_processing(predicted_trg_st))\n",
    "        print(\"Label:\", result_pred['label'])\n",
    "        print(\"Confidence Score:\", result_pred['score'])\n",
    "\n",
    "        if (result_trg['label'] != result_pred['label']):\n",
    "            correct_count += 1\n",
    "\n",
    "        #LM Score\n",
    "        gpt_lm_score = snt_ev.lm_score(post_processing(predicted_trg_st))\n",
    "        print(\"LM Score:\", gpt_lm_score)\n",
    "        lm_scores.append(gpt_lm_score)\n",
    "\n",
    "        #Similarity Score\n",
    "        similarity_score = snt_ev.similarity(post_processing(trg), post_processing(predicted_trg_st))\n",
    "        print('Similarity Score: ', similarity_score)\n",
    "        similarity_scores.append(similarity_score)\n",
    "        similarity_score_masked = snt_ev.similarity(snt_ev.mask_polarity(post_processing(trg)), snt_ev.mask_polarity(post_processing(predicted_trg_st)))\n",
    "        print('Masked Similarity Score: ', similarity_score_masked)\n",
    "        masked_similarity_scores.append(similarity_score_masked)\n",
    "        ###\n",
    "\n",
    "        print('\\n', flush=True)\n",
    "\n",
    "    lm_scores_mean = sum(lm_scores) / len(lm_scores)\n",
    "    similarity_scores_mean = sum(similarity_scores) / len(similarity_scores)\n",
    "    masked_similarity_scores_mean = sum(masked_similarity_scores) / len(masked_similarity_scores)\n",
    "\n",
    "\n",
    "    return correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean\n",
    "\n",
    "\n",
    "\n",
    "def imdb_op(data, decoder_flag):\n",
    "    correct_count = 0\n",
    "    lm_scores = []\n",
    "    similarity_scores = []\n",
    "    masked_similarity_scores = []\n",
    "    with open('imdb_outputours2.txt', 'w') as imdb_f:\n",
    "        for idx in range(1000):\n",
    "            example_idx = idx\n",
    "\n",
    "            src = vars(data.examples[example_idx])['src']\n",
    "            trg = vars(data.examples[example_idx])['trg']\n",
    "\n",
    "\n",
    "        \n",
    "            predicted_trg_st, attention = translate_sentence(src, SRC, TRG, model, device, 50, decoder_flag, True)\n",
    "\n",
    "            imdb_f.write(post_processing(predicted_trg_st)+'\\n')\n",
    "imdb_op(test_neg_data, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611c9fd-adf8-4e2c-b345-200ba63f0003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(best_valid_checkpoint['state_dict'])\n",
    "# print(\"Training Positive data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# evaluation(train_pos_data, 'pos')\n",
    "# print('##############################################################################################################')\n",
    "# print(\"Training Negative data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# evaluation(train_neg_data, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d163c6-6a86-464a-9cb4-383fb277f357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(best_valid_checkpoint['state_dict'])\n",
    "# print(\"Testing Positive data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# #evaluation(test_pos_data, 'pos')\n",
    "# correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean = evaluation(test_pos_data, 'pos')\n",
    "# print(correct_count)\n",
    "# print(lm_scores_mean)\n",
    "# print(similarity_scores_mean)\n",
    "# print(masked_similarity_scores_mean)\n",
    "print('##############################################################################################################')\n",
    "print(\"Testing Negative data\", flush=True)\n",
    "print('##############################################################################################################')\n",
    "#evaluation(test_neg_data, 'neg')\n",
    "#correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean = evaluation(test_neg_data, 'neg')\n",
    "#print(correct_count)\n",
    "#print(lm_scores_mean)\n",
    "#print(correct_count)\n",
    "#print(lm_scores_mean)\n",
    "#print(similarity_scores_mean)\n",
    "#print(masked_similarity_scores_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
