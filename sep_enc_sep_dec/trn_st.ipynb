{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72859eda-37a2-499d-adf2-17ae996212c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "import SentimentTransfer_Evaluations as snt_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e76d04-6803-4cae-b930-dbb8cf231fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = open(\"config.json\")\n",
    "config_obj = json.load(config_file)\n",
    "\n",
    "json_dumps = json.dumps(config_obj)\n",
    "print(json_dumps, flush=True)\n",
    "\n",
    "dir = config_obj[\"dir\"]\n",
    "field_fix_length = config_obj[\"field_fix_length\"]\n",
    "\n",
    "train_pos_data_path = config_obj[\"train_pos_data_path\"]\n",
    "train_neg_data_path = config_obj[\"train_neg_data_path\"]\n",
    "valid_pos_data_path = config_obj[\"valid_pos_data_path\"]\n",
    "valid_neg_data_path = config_obj[\"valid_neg_data_path\"]\n",
    "test_pos_data_path = config_obj[\"test_pos_data_path\"]\n",
    "test_neg_data_path = config_obj[\"test_neg_data_path\"]\n",
    "\n",
    "src_ext = config_obj[\"src_ext\"]\n",
    "trg_ext = config_obj[\"trg_ext\"]\n",
    "vocab_min_freq = config_obj[\"vocab_min_freq\"]\n",
    "vocab_max_size = config_obj[\"vocab_max_size\"]\n",
    "batch_size = config_obj[\"batch_size\"]\n",
    "encoder_max_length = config_obj[\"encoder_max_length\"]\n",
    "decoder_max_length = config_obj[\"decoder_max_length\"]\n",
    "style_num_embeddings = config_obj[\"style_num_embeddings\"]\n",
    "style_embedding_dim = config_obj[\"style_embedding_dim\"]\n",
    "hid_dim_enc = config_obj[\"hid_dim_enc\"]\n",
    "hid_dim_dec = config_obj[\"hid_dim_dec\"]\n",
    "enc_layers = config_obj[\"enc_layers\"]\n",
    "dec_layers = config_obj[\"dec_layers\"]\n",
    "enc_heads = config_obj[\"enc_heads\"]\n",
    "dec_heads = config_obj[\"dec_heads\"]\n",
    "enc_pf_dim = config_obj[\"enc_pf_dim\"]\n",
    "dec_pf_dim = config_obj[\"dec_pf_dim\"]\n",
    "enc_dropout = config_obj[\"enc_dropout\"]\n",
    "dec_dropout = config_obj[\"dec_dropout\"]\n",
    "learning_rate = config_obj[\"learning_rate\"]\n",
    "add_loss_t_s_start_epoch = config_obj[\"add_loss_t_s_start_epoch\"]\n",
    "add_loss_t_s_end_epoch = config_obj[\"add_loss_t_s_end_epoch\"]\n",
    "add_loss_t_w_start_epoch = config_obj[\"add_loss_t_w_start_epoch\"]\n",
    "add_loss_t_w_end_epoch = config_obj[\"add_loss_t_w_end_epoch\"]\n",
    "add_loss_s_w_start_epoch = config_obj[\"add_loss_s_w_start_epoch\"]\n",
    "add_loss_s_w_end_epoch = config_obj[\"add_loss_s_w_end_epoch\"]\n",
    "add_loss_t_s_w_start_epoch = config_obj[\"add_loss_t_s_w_start_epoch\"]\n",
    "add_loss_t_s_w_end_epoch = config_obj[\"add_loss_t_s_w_end_epoch\"]\n",
    "alt_loss_t_s_start_epoch = config_obj[\"alt_loss_t_s_start_epoch\"]\n",
    "alt_loss_t_s_end_epoch = config_obj[\"alt_loss_t_s_end_epoch\"]\n",
    "alt_loss_t_w_start_epoch = config_obj[\"alt_loss_t_w_start_epoch\"]\n",
    "alt_loss_t_w_end_epoch = config_obj[\"alt_loss_t_w_end_epoch\"]\n",
    "alt_loss_s_w_start_epoch = config_obj[\"alt_loss_s_w_start_epoch\"]\n",
    "alt_loss_s_w_end_epoch = config_obj[\"alt_loss_s_w_end_epoch\"]\n",
    "alt_loss_t_s_w_start_epoch = config_obj[\"alt_loss_t_s_w_start_epoch\"]\n",
    "alt_loss_t_s_w_end_epoch = config_obj[\"alt_loss_t_s_w_end_epoch\"]\n",
    "translation_loss_start_epoch = config_obj[\"translation_loss_start_epoch\"]\n",
    "translation_loss_end_epoch = config_obj[\"translation_loss_end_epoch\"]\n",
    "style_loss_start_epoch = config_obj[\"style_loss_start_epoch\"]\n",
    "style_loss_end_epoch = config_obj[\"style_loss_end_epoch\"]\n",
    "words_style_loss_start_epoch = config_obj[\"words_style_loss_start_epoch\"]\n",
    "words_style_loss_end_epoch = config_obj[\"words_style_loss_end_epoch\"]\n",
    "\n",
    "num_epochs = config_obj[\"num_epochs\"]\n",
    "clip = config_obj[\"clip\"]\n",
    "early_stop_lookout = config_obj[\"early_stop_lookout\"]\n",
    "another_early_stop_lookout = config_obj[\"another_early_stop_lookout\"]\n",
    "add_loss_t_s = config_obj[\"add_loss_t_s\"]\n",
    "add_loss_t_w = config_obj[\"add_loss_t_w\"]\n",
    "add_loss_s_w = config_obj[\"add_loss_s_w\"]\n",
    "add_loss_t_s_w = config_obj[\"add_loss_t_s_w\"]\n",
    "alt_loss_t_s = config_obj[\"alt_loss_t_s\"]\n",
    "alt_loss_t_w = config_obj[\"alt_loss_t_w\"]\n",
    "alt_loss_s_w = config_obj[\"alt_loss_s_w\"]\n",
    "alt_loss_t_s_w = config_obj[\"alt_loss_t_s_w\"]\n",
    "t_loss = config_obj[\"translation_loss\"]\n",
    "s_s_loss = config_obj[\"style_loss\"]\n",
    "w_s_loss = config_obj[\"words_style_loss\"]\n",
    "v_p_loss = config_obj[\"vocab_prob_loss\"]\n",
    "v_p_loss_start_epoch = config_obj[\"vocab_prob_loss_start_epoch\"]\n",
    "v_p_loss_end_epoch = config_obj[\"vocab_prob_loss_end_epoch\"]\n",
    "add_loss_t_v = config_obj[\"add_loss_t_v\"]\n",
    "add_loss_t_v_start_epoch = config_obj[\"add_loss_t_v_start_epoch\"]\n",
    "add_loss_t_v_end_epoch = config_obj[\"add_loss_t_v_end_epoch\"]\n",
    "alt_loss_t_v = config_obj[\"alt_loss_t_v\"]\n",
    "alt_loss_t_v_start_epoch = config_obj[\"alt_loss_t_v_start_epoch\"]\n",
    "alt_loss_t_v_end_epoch = config_obj[\"alt_loss_t_v_end_epoch\"]\n",
    "translation_loss_weight = config_obj[\"translation_loss_weight\"]\n",
    "vocab_loss_weight = config_obj[\"vocab_loss_weight\"]\n",
    "ss_loss_weight = config_obj[\"ss_loss_weight\"]\n",
    "ws_loss_weight = config_obj[\"ws_loss_weight\"]\n",
    "check_best_after_epoch = config_obj[\"check_best_after_epoch\"]\n",
    "check_best_after_epoch2 = config_obj[\"check_best_after_epoch2\"]\n",
    "specific_epoch_checkpoint = config_obj[\"specific_epoch_checkpoint\"]\n",
    "debug = config_obj[\"debug\"]\n",
    "style_cond = config_obj[\"style_cond\"]\n",
    "is_only_evaluation = config_obj[\"is_only_evaluation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d26de92-4602-47cb-8b4f-0eccb338e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694177e-2cc8-4a02-a58d-6fcc8725975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d33e67-5e66-4820-b680-e9e45bd0407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_field(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return dill.load(f)\n",
    "\n",
    "SRC = None\n",
    "TRG = None\n",
    "if os.path.exists(dir+'/src.field'):\n",
    "    SRC = load_field(os.path.join(dir, 'src.field'))\n",
    "if os.path.exists(dir+'/trg.field'):\n",
    "    TRG = load_field(os.path.join(dir, 'trg.field'))\n",
    "else:\n",
    "    SRC = Field(tokenize = tokenize_de,\n",
    "                # tokenize = 'spacy',\n",
    "                # tokenizer_language='en',\n",
    "                init_token='<sos>',\n",
    "                eos_token = '<eos>',\n",
    "                lower = True,\n",
    "                batch_first = True,\n",
    "                fix_length=field_fix_length)\n",
    "\n",
    "    TRG = Field(tokenize = tokenize_en,\n",
    "                # tokenize = 'spacy',\n",
    "                # tokenizer_language='de',\n",
    "                init_token='<sos>',\n",
    "                eos_token = '<eos>',\n",
    "                lower = True,\n",
    "                batch_first = True,\n",
    "                fix_length=field_fix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326b2973-9aa7-41f9-96e2-3df98e482999",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_data = TranslationDataset(\n",
    "    path=train_pos_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "train_neg_data = TranslationDataset(\n",
    "    path=train_neg_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "valid_pos_data = TranslationDataset(\n",
    "    path=valid_pos_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "valid_neg_data = TranslationDataset(\n",
    "    path=valid_neg_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "test_pos_data = TranslationDataset(\n",
    "    path=test_pos_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "test_neg_data = TranslationDataset(\n",
    "    path=test_neg_data_path,\n",
    "    exts=(src_ext, trg_ext),\n",
    "    fields=(SRC, TRG),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed5871e-5ac8-4921-b11b-ec7499accaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_pos_data.examples)}\", flush=True)\n",
    "print(f\"Number of training examples: {len(train_neg_data.examples)}\", flush=True)\n",
    "\n",
    "print(f\"Number of validation examples: {len(valid_pos_data.examples)}\", flush=True)\n",
    "print(f\"Number of validation examples: {len(valid_neg_data.examples)}\", flush=True)\n",
    "\n",
    "print(f\"Number of testing examples: {len(test_pos_data.examples)}\", flush=True)\n",
    "print(f\"Number of testing examples: {len(test_neg_data.examples)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76154bc8-441a-4cea-9657-149c328ea5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_field(field,name, savedir=dir):\n",
    "    save_path = os.path.join(savedir, f\"{name}.field\")\n",
    "    with open(save_path, 'wb') as fout:\n",
    "        dill.dump(field, fout)\n",
    "\n",
    "def save_vocab(field, name, savedir=dir):\n",
    "    save_path = os.path.join(savedir, f\"{name}_vocab.txt\")\n",
    "    with open(save_path, 'w') as fout:\n",
    "        for w in field.vocab.itos:\n",
    "            fout.write(w + '\\n')\n",
    "\n",
    "if os.path.exists(dir+'/src.field') and os.path.exists(dir+'/src.vocab'):\n",
    "    pass\n",
    "else:\n",
    "    SRC.build_vocab(train_pos_data, train_neg_data, min_freq=vocab_min_freq, max_size=vocab_max_size)\n",
    "    save_field(SRC, \"src\", dir)\n",
    "    save_vocab(SRC, \"src\", dir)\n",
    "if os.path.exists(dir+'/trg.field') and os.path.exists(dir+'/trg.vocab'):\n",
    "    pass\n",
    "else:\n",
    "    TRG.build_vocab(train_pos_data, train_neg_data, min_freq=vocab_min_freq, max_size=vocab_max_size)\n",
    "    save_field(TRG, \"trg\", dir)\n",
    "    save_vocab(TRG, \"trg\", dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14999df9-a68c-4381-b326-f2496df2d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique tokens in source vocabulary: {len(SRC.vocab)}\", flush=True)\n",
    "print(f\"Unique tokens in target vocabulary: {len(TRG.vocab)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3b1b1-50e0-464e-9ab6-2abdccbaeb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46a6400-4b91-4482-a69b-4f96967aa228",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249fde9d-1e9f-45e0-a257-7d9edb20554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_iterator, train_neg_iterator, valid_pos_iterator, valid_neg_iterator, test_pos_iterator, test_neg_iterator = BucketIterator.splits(\n",
    "    (train_pos_data, train_neg_data, valid_pos_data, valid_neg_data, test_pos_data, test_neg_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key= lambda x: len(x.src),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6db1c0-eb27-4c72-9162-151faba3ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=encoder_max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch size, src len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, src len]\n",
    "\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ada28b-9228-4210-90a0-dad94e060212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch size, src len, hid dim]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bab38-0771-4075-98f7-ee9131dae25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query = [batch size, query len, hid dim]\n",
    "        # key = [batch size, key len, hid dim]\n",
    "        # value = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q = [batch size, query len, hid dim]\n",
    "        # K = [batch size, key len, hid dim]\n",
    "        # V = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q = [batch size, n heads, query len, head dim]\n",
    "        # K = [batch size, n heads, key len, head dim]\n",
    "        # V = [batch size, n heads, value len, head dim]\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # energy = [batch size, n heads, query len, key len]\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention = [batch size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x = [batch size, n heads, query len, head dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x = [batch size, query len, n heads, head dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865f9679-d13a-46ce-bcb4-4d8086300081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x = [batch size, seq len, pf dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef4a3b9-11be-41e7-8c5b-5b95ca3a666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=decoder_max_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, trg len]\n",
    "\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfaf6b3-1ed2-48e4-bcf1-47cc25d35dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9662200-5657-4989-8830-d8da2151f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 pos_decoder,\n",
    "                 neg_decoder,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.pos_decoder = pos_decoder\n",
    "        self.neg_decoder = neg_decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src = [batch size, src len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        # trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg, decoder_flag):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "\n",
    "        if(decoder_flag == 'pos'):\n",
    "            output, attention = self.pos_decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        elif(decoder_flag == 'neg'):\n",
    "            output, attention = self.neg_decoder(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed6739-84dc-4613-9465-ff703a79c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "#HID_DIM = 512\n",
    "HID_DIM_ENC = hid_dim_enc\n",
    "HID_DIM_DEC = hid_dim_dec\n",
    "ENC_LAYERS = enc_layers\n",
    "DEC_LAYERS = dec_layers\n",
    "ENC_HEADS = enc_heads\n",
    "DEC_HEADS = dec_heads\n",
    "ENC_PF_DIM = enc_pf_dim\n",
    "DEC_PF_DIM = dec_pf_dim\n",
    "ENC_DROPOUT = enc_dropout\n",
    "DEC_DROPOUT = dec_dropout\n",
    "\n",
    "enc = Encoder(INPUT_DIM,\n",
    "              HID_DIM_ENC,\n",
    "              ENC_LAYERS,\n",
    "              ENC_HEADS,\n",
    "              ENC_PF_DIM,\n",
    "              ENC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "pos_dec = Decoder(OUTPUT_DIM,\n",
    "              HID_DIM_DEC,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)\n",
    "neg_dec = Decoder(OUTPUT_DIM,\n",
    "              HID_DIM_DEC,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74b91a-5702-472d-8a9d-bd3322ca1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f36ba9-221e-4b23-a934-22d11390fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(enc, pos_dec, neg_dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb23174-a570-481e-8701-2e9da8c9643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b8239-74a8-42e9-a2c2-488c67070c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "if is_only_evaluation == False:\n",
    "    model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e0412-4387-417b-8466-b2d64793487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = learning_rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d31f87-fe0d-4593-81f3-d4625355fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(dir+'/checkpoint_7.pt')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# save_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d83cb3-9c8c-4c33-9974-6d83bdfe6d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "def sws(target_senti, trg_tokens):\n",
    "    senti_word_score = []\n",
    "    i=0\n",
    "    while(i<len(trg_tokens)):\n",
    "        token=trg_tokens[i]\n",
    "        bpe_count = 1\n",
    "        if(token.find('@@')!=-1):\n",
    "            # import pudb\n",
    "            # pudb.set_trace()\n",
    "            token=token.replace('@@', '')\n",
    "            i+=1\n",
    "            while(i<len(trg_tokens)):\n",
    "                bpe_count += 1\n",
    "                token += trg_tokens[i]\n",
    "                if(trg_tokens[i].find('@@')!=-1):\n",
    "                    token = token.replace('@@', '')\n",
    "                else:\n",
    "                    break\n",
    "                i += 1\n",
    "        i += 1\n",
    "\n",
    "        score = senti_analyzer.polarity_scores(token)\n",
    "\n",
    "        if(target_senti == 1):\n",
    "            if(score['neu']==1.0):\n",
    "                senti_word_score = senti_word_score + [1] * bpe_count\n",
    "            elif(score['pos']==1.0):\n",
    "                senti_word_score = senti_word_score + [1] * bpe_count\n",
    "            elif(score['neg']==1.0):\n",
    "                senti_word_score = senti_word_score + [0] * bpe_count\n",
    "            else:\n",
    "                senti_word_score = senti_word_score + [1] * bpe_count\n",
    "        else:\n",
    "            if (score['neu'] == 1.0):\n",
    "                senti_word_score = senti_word_score + [0] * bpe_count\n",
    "            elif (score['pos'] == 1.0):\n",
    "                senti_word_score = senti_word_score + [1] * bpe_count\n",
    "            elif (score['neg'] == 1.0):\n",
    "                senti_word_score = senti_word_score + [0] * bpe_count\n",
    "            else:\n",
    "                senti_word_score = senti_word_score + [0] * bpe_count\n",
    "\n",
    "    if (len(senti_word_score) < 99):\n",
    "        if (target_senti == 0):\n",
    "            senti_word_score = senti_word_score + [0] * (99 - len(senti_word_score))\n",
    "        else:\n",
    "            senti_word_score = senti_word_score + [1] * (99 - len(senti_word_score))\n",
    "\n",
    "    return senti_word_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10a0f9a-a97d-47c1-a036-26ded1188b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trg_vocab_loss():\n",
    "    pos = [] #pos values in vocab 1, neu and neg is 0\n",
    "    neg = [] #neg values in vocab 1, neu and pos is 0\n",
    "    for word in TRG.vocab.itos:\n",
    "        polarity = senti_analyzer.polarity_scores(word)\n",
    "        if (polarity['neg'] == 1.0):\n",
    "            pos.append(0.0)\n",
    "            neg.append(1.0)\n",
    "        elif (polarity['pos'] == 1.0):\n",
    "            pos.append(1.0)\n",
    "            neg.append(0.0)\n",
    "        else:\n",
    "            pos.append(0.0)\n",
    "            neg.append(0.0)\n",
    "    return pos, neg\n",
    "\n",
    "def loss_translation(criterion, output, trg):\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.contiguous().view(-1, output_dim)\n",
    "    trg = trg[:, 1:].contiguous().view(-1)\n",
    "    # output = [batch size * trg len - 1, output dim]\n",
    "    # trg = [batch size * trg len - 1]\n",
    "    translation_loss = criterion(output, trg)\n",
    "    return translation_loss\n",
    "\n",
    "def trg_initial_style_ops(trg):\n",
    "    trg_copy = trg.detach().clone()\n",
    "    trg_ops_list = []\n",
    "    for i, x in enumerate(trg_copy.cpu().numpy()):\n",
    "        if (x[0] == 3):\n",
    "            x[0] = 4\n",
    "        elif (x[0] == 4):\n",
    "            x[0] = 3\n",
    "        trg_ops_list.append(x)\n",
    "    return torch.from_numpy(np.array(trg_ops_list)).to(device)\n",
    "\n",
    "def prepare_style_embeddding_tensors(src):\n",
    "    #polarity = []\n",
    "    polarity_ops = []\n",
    "    style_tok_list_enc = []\n",
    "    style_tok_list_dec = []\n",
    "    style_tok_list_dec_ops = []\n",
    "    #senti_trg_list_ops2 = []\n",
    "\n",
    "    for i, x in enumerate(src.cpu().numpy()):\n",
    "        if (x[0] == 3): #positive\n",
    "            #polarity.append(1) #0 means negative, 1 means positive, simple translation, pos -> pos and neg -> neg, as it is\n",
    "            polarity_ops.append(0) # for style transfer, pos -> neg and neg -> pos\n",
    "            #senti_trg_list_ops2.append([0 for i in range(99)])\n",
    "            style_tok_list_enc.append(pos_list_enc)\n",
    "            style_tok_list_dec.append(pos_list_dec)\n",
    "            style_tok_list_dec_ops.append(neg_list_dec)\n",
    "        elif (x[0] == 4): #negative\n",
    "            #polarity.append(0)\n",
    "            polarity_ops.append(1)\n",
    "            #senti_trg_list_ops2.append([1 for i in range(99)])\n",
    "            style_tok_list_enc.append(neg_list_enc)\n",
    "            style_tok_list_dec.append(neg_list_dec)\n",
    "            style_tok_list_dec_ops.append(pos_list_dec) # for style transfer, pos -> neg and neg -> pos\n",
    "\n",
    "    return polarity_ops, torch.from_numpy(np.array(style_tok_list_enc)).to(device), torch.from_numpy(np.array(style_tok_list_dec)).to(device), torch.from_numpy(np.array(style_tok_list_dec_ops)).to(device)\n",
    "def pred_texts(output, field):\n",
    "    texts = []\n",
    "    for i, x in enumerate(output.cpu().detach().numpy()):\n",
    "        gen_trg_indexes = []\n",
    "        for x_ind in x:\n",
    "            single_tensor = torch.from_numpy(x_ind)\n",
    "            gen_pred_token = single_tensor.argmax().item()\n",
    "            gen_trg_indexes.append(gen_pred_token)\n",
    "            if gen_pred_token == field.vocab.stoi[field.eos_token]:\n",
    "                break\n",
    "        trg_tokens = [field.vocab.itos[i] for i in gen_trg_indexes]\n",
    "        texts.append(trg_tokens)\n",
    "    return texts\n",
    "def batch_to_texts(batch, src_field, trg_field):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    src_texts = []\n",
    "    trg_texts = []\n",
    "    for idxs in src.cpu().numpy():\n",
    "        src_texts.append([src_field.vocab.itos[idx] for idx in idxs])\n",
    "    for idxs in trg.cpu().numpy():\n",
    "        trg_texts.append([trg_field.vocab.itos[idx] for idx in idxs])\n",
    "    return src_texts, trg_texts\n",
    "\n",
    "def style_loss_sentence_level(polarity_ops, output_oppos):\n",
    "    #bce_logit_loss = nn.BCEWithLogitsLoss()\n",
    "    bce_loss = nn.BCELoss()\n",
    "    senti_score_list = []\n",
    "    for i, x in enumerate(output_oppos.cpu().detach().numpy()):\n",
    "        gen_trg_indexes = []\n",
    "        for x_ind in x:\n",
    "            single_tensor = torch.from_numpy(x_ind)\n",
    "            gen_pred_token = single_tensor.argmax().item()\n",
    "            gen_trg_indexes.append(gen_pred_token)\n",
    "            if gen_pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                break\n",
    "        trg_tokens = [TRG.vocab.itos[i] for i in gen_trg_indexes]\n",
    "\n",
    "        senti_pred_score = None #sentiment.predict_sentiment(sentiment.model, sentiment.tokenizer, post_processing(trg_tokens))\n",
    "        senti_score_list.append(senti_pred_score)\n",
    "\n",
    "    senti_loss2 = bce_loss(torch.tensor(np.array(senti_score_list), requires_grad=True).to(device), torch.tensor(np.array(polarity_ops)).double().to(device))\n",
    "    return senti_loss2\n",
    "\n",
    "def style_loss_word_level_simple(polarity_ops, output_oppos):\n",
    "    senti_word_scores = []\n",
    "    senti_trg_tensor2 = [[i]*99 for i in polarity_ops]\n",
    "    criterion = nn.BCELoss()\n",
    "    for i, x in enumerate(output_oppos.cpu().detach().numpy()):\n",
    "        gen_trg_indexes = []\n",
    "        for x_ind in x:\n",
    "            single_tensor = torch.from_numpy(x_ind)\n",
    "            gen_pred_token = single_tensor.argmax().item()\n",
    "            gen_trg_indexes.append(gen_pred_token)\n",
    "            if gen_pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                break\n",
    "        trg_tokens = [TRG.vocab.itos[i] for i in gen_trg_indexes]\n",
    "        senti_word_scores.append(sws(polarity_ops[i], trg_tokens))\n",
    "\n",
    "    words_style_loss = criterion(torch.tensor(np.array(senti_word_scores, dtype=float), requires_grad=True).to(device), torch.tensor(np.array(senti_trg_tensor2, dtype=float)).to(device))\n",
    "    return words_style_loss\n",
    "\n",
    "def simple_loss_prob_vocab(output_oppos, senti_trg_list_ops, trg_vocab_pos, trg_vocab_neg):\n",
    "    trg_metric = []\n",
    "    output_oppos_logits = nn.Sigmoid()(output_oppos)\n",
    "    for i, x in enumerate(output_oppos_logits.cpu().detach().numpy()):\n",
    "        target_senti = senti_trg_list_ops[i]\n",
    "        temp = []\n",
    "        for j, x_ind in enumerate(x):\n",
    "            single_tensor = torch.from_numpy(x_ind)\n",
    "            gen_pred_token = single_tensor.argmax().item()\n",
    "            token = TRG.vocab.itos[gen_pred_token]\n",
    "            polarity = senti_analyzer.polarity_scores(token.replace('@@',''))\n",
    "            if (target_senti == 1): # n -> p\n",
    "                if (polarity['neg'] == 1.0): # if any token is neg, goal is to shift towards: pred(pos token) -> trg(1), pred(neg token) -> trg(0), pred(neu token) -> trg(0)\n",
    "                    temp.append(trg_vocab_pos) # trg_vocab_pos: means all pos token in vocab is 1, neg and neu is 0\n",
    "                else:\n",
    "                    temp.append(x_ind)\n",
    "            else: # p -> n\n",
    "                if (polarity['pos'] == 1.0): # if any token is pos, goal is to shift towards: pred(neg token) -> trg(1), pred(pos token) -> trg(0), pred(neu token) -> trg(0)\n",
    "                    temp.append(trg_vocab_neg) # trg_vocab_neg: means all neg token in vocab is 1, pos and neu is 0\n",
    "                else:\n",
    "                    temp.append(x_ind)\n",
    "        trg_metric.append(temp)\n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(output_oppos_logits.double(), torch.from_numpy(np.array(trg_metric, dtype=float)).to(device))\n",
    "    return loss\n",
    "\n",
    "def overall_loss(epoch, translation_loss, senti_loss2, words_style_loss, loss_prob_vocab, alt_loss_bool):\n",
    "    new_loss = None\n",
    "    if add_loss_t_s == True and epoch >= add_loss_t_s_start_epoch and epoch <= add_loss_t_s_end_epoch:\n",
    "        new_loss = translation_loss_weight*translation_loss + ss_loss_weight*senti_loss2\n",
    "    if add_loss_t_w == True and epoch >= add_loss_t_w_start_epoch and epoch <= add_loss_t_w_end_epoch:\n",
    "        new_loss = translation_loss_weight*translation_loss + ws_loss_weight*words_style_loss\n",
    "    if add_loss_s_w == True and epoch >= add_loss_s_w_start_epoch and epoch <= add_loss_s_w_end_epoch:\n",
    "        new_loss = senti_loss2 + words_style_loss\n",
    "    if add_loss_t_s_w == True and epoch >= add_loss_t_s_w_start_epoch and epoch <= add_loss_t_s_w_end_epoch:\n",
    "        new_loss = translation_loss + senti_loss2 + words_style_loss\n",
    "    if add_loss_t_v == True and epoch >= add_loss_t_v_start_epoch and epoch <= add_loss_t_v_end_epoch:\n",
    "        new_loss = translation_loss_weight*translation_loss + vocab_loss_weight*loss_prob_vocab\n",
    "\n",
    "    if alt_loss_t_s == True and epoch >= alt_loss_t_s_start_epoch and epoch <= alt_loss_t_s_end_epoch:\n",
    "        if (alt_loss_bool == True):\n",
    "            new_loss = translation_loss_weight*translation_loss\n",
    "        else:\n",
    "            new_loss = ss_loss_weight*senti_loss2\n",
    "    if alt_loss_t_w == True and epoch >= alt_loss_t_w_start_epoch and epoch <= alt_loss_t_w_end_epoch:\n",
    "        if (alt_loss_bool == True):\n",
    "            new_loss = translation_loss_weight*translation_loss\n",
    "        else:\n",
    "            new_loss = ws_loss_weight*words_style_loss\n",
    "\n",
    "    if alt_loss_s_w == True and epoch >= alt_loss_s_w_start_epoch and epoch <= alt_loss_s_w_end_epoch:\n",
    "        if (alt_loss_bool == True):\n",
    "            new_loss = senti_loss2\n",
    "        else:\n",
    "            new_loss = words_style_loss\n",
    "\n",
    "    if alt_loss_t_v == True and epoch >= alt_loss_t_v_start_epoch and epoch <= alt_loss_t_v_end_epoch:\n",
    "        if (alt_loss_bool == True):\n",
    "            new_loss = translation_loss\n",
    "        else:\n",
    "            new_loss = loss_prob_vocab\n",
    "\n",
    "    # if alt_loss_t_s_w == True and epoch >= alt_loss_t_s_w_start_epoch and epoch <= alt_loss_t_s_w_end_epoch:\n",
    "    #     if (alt_loss_bool == True and alt_loss_bool2 == False):\n",
    "    #         new_loss = translation_loss\n",
    "    #         alt_loss_bool = False\n",
    "    #         alt_loss_bool2 = True\n",
    "    #     elif (alt_loss_bool == False and alt_loss_bool2 == True):\n",
    "    #         new_loss = senti_loss2\n",
    "    #         alt_loss_bool = True\n",
    "    #         alt_loss_bool2 = True\n",
    "    #     elif (alt_loss_bool == True and alt_loss_bool2 == True):\n",
    "    #         new_loss = words_style_loss\n",
    "    #         alt_loss_bool = True\n",
    "    #         alt_loss_bool2 = False\n",
    "\n",
    "    if t_loss == True and epoch >= translation_loss_start_epoch and epoch <= translation_loss_end_epoch:\n",
    "        new_loss = translation_loss\n",
    "    if s_s_loss == True and epoch >= style_loss_start_epoch and epoch <= style_loss_end_epoch:\n",
    "        new_loss = senti_loss2\n",
    "    if w_s_loss == True and epoch >= words_style_loss_start_epoch and epoch <= words_style_loss_end_epoch:\n",
    "        new_loss = words_style_loss\n",
    "    if v_p_loss == True and epoch >= v_p_loss_start_epoch and epoch <= v_p_loss_end_epoch:\n",
    "        new_loss = loss_prob_vocab\n",
    "    return new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea9aeba-34b3-450a-9984-8708332f0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list_enc = [3 for i in range(100)]\n",
    "neg_list_enc = [4 for i in range(100)]\n",
    "\n",
    "pos_list_dec = [3 for i in range(99)]\n",
    "neg_list_dec = [4 for i in range(99)]\n",
    "\n",
    "def calculate_output_loss(epoch, model, pos_iterator, neg_iterator, criterion, is_eval, optimizer=None, clip=None):\n",
    "    epoch_translation_loss = 0\n",
    "\n",
    "    total_no_batches = 0\n",
    "\n",
    "    #itrs = [pos_iterator, neg_iterator]\n",
    "    itrs = [neg_iterator]\n",
    "    for idx, itr in enumerate(itrs):\n",
    "        if (idx == 0):\n",
    "            decoder_flag = 'neg'\n",
    "        else:\n",
    "            decoder_flag = 'pos'\n",
    "        for i, batch in enumerate(itr):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            # import pudb\n",
    "            # pudb.set_trace()\n",
    "\n",
    "            # if epoch >= specific_epoch_checkpoint and debug == True:\n",
    "            #     src_texts, trg_texts = batch_to_texts(batch, SRC, TRG)\n",
    "\n",
    "            if is_eval==False:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(src, trg[:, :-1], decoder_flag)\n",
    "            # if epoch >= specific_epoch_checkpoint and debug == True:\n",
    "            #     predicted_texts = pred_texts(output, TRG)\n",
    "\n",
    "            translation_loss = loss_translation(criterion, output, trg)\n",
    "\n",
    "            if is_eval == False:\n",
    "                translation_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "\n",
    "            if(translation_loss is not None):\n",
    "                epoch_translation_loss += translation_loss.item()\n",
    "            total_no_batches +=1\n",
    "\n",
    "    epoch_translation_loss_avg = epoch_translation_loss / total_no_batches\n",
    "\n",
    "    return epoch_translation_loss_avg\n",
    "\n",
    "def train(epoch, model, pos_iterator, neg_iterator, criterion, is_eval, optimizer=None, clip=None):\n",
    "    if is_eval == False:\n",
    "        model.train()\n",
    "        epoch_translation_loss = calculate_output_loss(epoch, model, pos_iterator, neg_iterator, criterion, False, optimizer, clip)\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            epoch_translation_loss = calculate_output_loss(epoch, model, pos_iterator, neg_iterator, criterion, True)\n",
    "    return epoch_translation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cb550a-44e1-430c-bced-ff6e666a18a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def post_processing(text_list):\n",
    "    repl_list = {'@@ ': '', '<eos>':''}\n",
    "    text_str = ' '.join(text_list)\n",
    "    return functools.reduce(lambda a, kv: a.replace(*kv), repl_list.items(), text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca50538-ac9f-40ce-9501-0948de889204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eb00d3-f768-44a0-899d-ed987fe6eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_only_evaluation == False:\n",
    "    N_EPOCHS = num_epochs\n",
    "    CLIP = clip\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    valid_loss_before = float('inf')\n",
    "\n",
    "    early_stop_cnt = 0\n",
    "    early_stop_lookout = early_stop_lookout\n",
    "    early_stop=False\n",
    "\n",
    "    another_early_stop_cnt = 0\n",
    "    another_early_stop_lookout = another_early_stop_lookout\n",
    "    another_early_stop=False\n",
    "\n",
    "    best_epoch_no = 0\n",
    "\n",
    "    #for epoch in range(save_epoch, N_EPOCHS):\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_translation_loss = train(epoch, model, train_pos_iterator, train_neg_iterator, criterion, False, optimizer, CLIP)\n",
    "        valid_translation_loss = train(epoch, model, valid_pos_iterator, valid_neg_iterator, criterion, True)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, dir + f'/latest_checkpoint.pt')\n",
    "        if(epoch == specific_epoch_checkpoint):\n",
    "            torch.save(checkpoint, dir + f'/checkpoint_{specific_epoch_checkpoint}.pt')\n",
    "\n",
    "        if(epoch > check_best_after_epoch):\n",
    "            #Early_Stop\n",
    "            if valid_translation_loss < best_valid_loss or valid_translation_loss < valid_loss_before:\n",
    "                early_stop_cnt = 0\n",
    "                early_stop = False\n",
    "\n",
    "            elif valid_translation_loss >= best_valid_loss or valid_translation_loss >= valid_loss_before:\n",
    "                early_stop_cnt += 1\n",
    "                early_stop = True\n",
    "\n",
    "            # Another Early_Stop based on only best valid translation_loss\n",
    "            if epoch > check_best_after_epoch2 :\n",
    "                if valid_translation_loss < best_valid_loss:\n",
    "                    another_early_stop_cnt = 0\n",
    "                    another_early_stop = False\n",
    "\n",
    "                elif valid_translation_loss >= best_valid_loss:\n",
    "                    another_early_stop_cnt += 1\n",
    "                    another_early_stop = True\n",
    "\n",
    "\n",
    "            if valid_translation_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_translation_loss\n",
    "                best_epoch_no = epoch\n",
    "\n",
    "                torch.save(checkpoint, dir + '/best_valid_checkpoint.pt')\n",
    "                # if(epoch==4):\n",
    "                #     torch.save(checkpoint, dir+'/epoch5_checkpoint.pt')\n",
    "\n",
    "            valid_loss_before = valid_translation_loss\n",
    "\n",
    "\n",
    "        print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s', flush=True)\n",
    "\n",
    "        if(train_translation_loss!=0):\n",
    "            print(f'\\tTrain Translation Loss: {train_translation_loss:.7f}', flush=True)\n",
    "\n",
    "        if (valid_translation_loss != 0):\n",
    "            print(f'\\t Val. Translation Loss: {valid_translation_loss:.3f}', flush=True)\n",
    "\n",
    "        print(f'\\t Till now Best Val. Loss: {best_valid_loss:.3f} found on {best_epoch_no+1} epoch ', flush=True)\n",
    "\n",
    "        if early_stop==True :\n",
    "            print(f'EarlyStopping counter (1st way): {early_stop_cnt} out of {early_stop_lookout}', flush=True)\n",
    "\n",
    "        if another_early_stop==True :\n",
    "            print(f'EarlyStopping counter (2nd way): {another_early_stop_cnt} out of {another_early_stop_lookout}', flush=True)\n",
    "\n",
    "        print('\\n', flush=True)\n",
    "\n",
    "\n",
    "        if early_stop_cnt == early_stop_lookout:\n",
    "            print('Early Stoping (1st way)...', flush=True)\n",
    "            break\n",
    "        if another_early_stop_cnt == another_early_stop_lookout:\n",
    "            print('Early Stoping (2nd way)...', flush=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea7a30-7705-4b3d-ac89-e563b4e0a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_checkpoint = torch.load(dir+'/latest_checkpoint.pt')\n",
    "best_valid_checkpoint = torch.load(dir+'/best_valid_checkpoint.pt')\n",
    "\n",
    "model.load_state_dict(best_valid_checkpoint['state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#epoch = checkpoint['epoch']\n",
    "\n",
    "test_loss = train(None, model, test_pos_iterator, test_neg_iterator, criterion, True)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844000f0-758f-4127-8b65-89c8503e77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def display_attention(sentence, translation, attention, n_heads=8, n_rows=4, n_cols=2):\n",
    "#     assert n_rows * n_cols == n_heads\n",
    "#\n",
    "#     fig = plt.figure(figsize=(15, 25))\n",
    "#\n",
    "#     for i in range(n_heads):\n",
    "#         ax = fig.add_subplot(n_rows, n_cols, i + 1)\n",
    "#\n",
    "#         _attention = attention.squeeze(0)[i].cpu().detach().numpy()\n",
    "#\n",
    "#         cax = ax.matshow(_attention, cmap='bone')\n",
    "#\n",
    "#         ax.tick_params(labelsize=12)\n",
    "#         ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in sentence] + ['<eos>'],\n",
    "#                            rotation=45)\n",
    "#         ax.set_yticklabels([''] + translation)\n",
    "#\n",
    "#         ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#         ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "#\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07059831-aec4-4411-9dc5-17ece8f2ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len, decoder_flag, is_st):\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    if(is_st == True):\n",
    "        if(decoder_flag=='pos'):\n",
    "            decoder_flag = 'neg'\n",
    "        elif(decoder_flag=='neg'):\n",
    "            decoder_flag = 'pos'\n",
    "\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, src_mask)\n",
    "\n",
    "    trg_indexes = [trg_field.vocab.stoi[tokens[0]]]\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if(decoder_flag == 'pos'):\n",
    "                output, attention = model.pos_decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "            elif(decoder_flag == 'neg'):\n",
    "                output, attention = model.neg_decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250f4e7-4b8f-460f-ab0d-3849bc0c0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display_attention(src, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3a425-4a79-45ba-bb27-0f41317f218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "senti_trg_list_ops = []\n",
    "def masked_sent(sent_list):\n",
    "    masked = snt_ev.mask_polarity(post_processing(sent_list))\n",
    "    return nltk.word_tokenize(masked)\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len, decoder_flag, is_st):\n",
    "    trgs = []\n",
    "    masked_trgs = []\n",
    "\n",
    "    pred_trgs = []\n",
    "    masked_pred_trgs = []\n",
    "\n",
    "    lengthy_idx = []\n",
    "    for idx, datum in enumerate(data):\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        if len(src) < 100 and len(trg) < 100:\n",
    "            pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, decoder_flag, is_st)\n",
    "\n",
    "            # cut off <eos> token\n",
    "            pred_trg = pred_trg[:-1]\n",
    "\n",
    "            pred_trgs.append(pred_trg)\n",
    "            masked_pred_trgs.append(masked_sent(pred_trg))\n",
    "            trgs.append([trg])\n",
    "            masked_trgs.append([masked_sent(trg)])\n",
    "        else:\n",
    "            lengthy_idx.append(idx)\n",
    "\n",
    "    # print(lengthy_idx)\n",
    "    # print(len(pred_trgs))\n",
    "    return bleu_score(pred_trgs, trgs), bleu_score(masked_pred_trgs, masked_trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db16e3-b55c-4b0d-83a8-9d5c45334665",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_score, masked_b_score = calculate_bleu(test_pos_data, SRC, TRG, model, device, 50, 'pos', True)\n",
    "print('\\n')\n",
    "print(f'Style Transfer: BLEU score on test pos data = {b_score*100:.5f}', flush=True)\n",
    "print(f'Style Transfer: Masked BLEU score on test pos data = {masked_b_score*100:.5f}', flush=True)\n",
    "b_score, masked_b_score = calculate_bleu(test_neg_data, SRC, TRG, model, device, 50, 'neg', True)\n",
    "print('\\n')\n",
    "print(f'Style Transfer: BLEU score on test neg data = {b_score*100:.5f}', flush=True)\n",
    "print(f'Style Transfer: Masked BLEU score on test neg data = {masked_b_score*100:.5f}', flush=True)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4caa9c2-907a-466f-a119-1e10193c2066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data, decoder_flag):\n",
    "    correct_count = 0\n",
    "    lm_scores = []\n",
    "    similarity_scores = []\n",
    "    masked_similarity_scores = []\n",
    "\n",
    "    for idx in range(1000):\n",
    "        example_idx = idx\n",
    "\n",
    "        src = vars(data.examples[example_idx])['src']\n",
    "        trg = vars(data.examples[example_idx])['trg']\n",
    "\n",
    "\n",
    "        predicted_trg_trn, attention = translate_sentence(src, SRC, TRG, model, device, 50, decoder_flag, False)\n",
    "        predicted_trg_st, attention = translate_sentence(src, SRC, TRG, model, device, 50, decoder_flag, True)\n",
    "\n",
    "        print(f'src = {post_processing(src)}', flush=True)\n",
    "\n",
    "        print(f'trg = {post_processing(trg)}', flush=True)\n",
    "        result_trg = snt_ev.senti_score(post_processing(trg))\n",
    "        print(\"Label:\", result_trg['label'])\n",
    "        print(\"Confidence Score:\", result_trg['score'])\n",
    "\n",
    "\n",
    "        print(f'style transfered predicted trg = {post_processing(predicted_trg_st)}', flush=True)\n",
    "\n",
    "        # Sentiment Score\n",
    "        result_pred = snt_ev.senti_score(post_processing(predicted_trg_st))\n",
    "        print(\"Label:\", result_pred['label'])\n",
    "        print(\"Confidence Score:\", result_pred['score'])\n",
    "\n",
    "        if (result_trg['label'] != result_pred['label']):\n",
    "            correct_count += 1\n",
    "\n",
    "        #LM Score\n",
    "        gpt_lm_score = snt_ev.lm_score(post_processing(predicted_trg_st))\n",
    "        print(\"LM Score:\", gpt_lm_score)\n",
    "        lm_scores.append(gpt_lm_score)\n",
    "\n",
    "        #Similarity Score\n",
    "        similarity_score = snt_ev.similarity(post_processing(trg), post_processing(predicted_trg_st))\n",
    "        print('Similarity Score: ', similarity_score)\n",
    "        similarity_scores.append(similarity_score)\n",
    "        similarity_score_masked = snt_ev.similarity(snt_ev.mask_polarity(post_processing(trg)), snt_ev.mask_polarity(post_processing(predicted_trg_st)))\n",
    "        print('Masked Similarity Score: ', similarity_score_masked)\n",
    "        masked_similarity_scores.append(similarity_score_masked)\n",
    "        ###\n",
    "\n",
    "        print('\\n', flush=True)\n",
    "\n",
    "    lm_scores_mean = sum(lm_scores) / len(lm_scores)\n",
    "    similarity_scores_mean = sum(similarity_scores) / len(similarity_scores)\n",
    "    masked_similarity_scores_mean = sum(masked_similarity_scores) / len(masked_similarity_scores)\n",
    "\n",
    "\n",
    "    return correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa6967-5144-422a-9389-53c176ecdfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(best_valid_checkpoint['state_dict'])\n",
    "# print(\"Training Positive data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# evaluation(train_pos_data, 'pos')\n",
    "# print('##############################################################################################################')\n",
    "# print(\"Training Negative data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# evaluation(train_neg_data, 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c95794-bfa4-4441-ad02-29db4583e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(best_valid_checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19888acf-e68e-4d3c-b1be-5ca85bd79295",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Positive data\", flush=True)\n",
    "print('##############################################################################################################')\n",
    "#evaluation(test_pos_data, 'pos')\n",
    "correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean = evaluation(test_pos_data, 'pos')\n",
    "print(correct_count)\n",
    "print(lm_scores_mean)\n",
    "print(similarity_scores_mean)\n",
    "print(masked_similarity_scores_mean)\n",
    "print('##############################################################################################################')\n",
    "print(\"Testing Negative data\", flush=True)\n",
    "print('##############################################################################################################')\n",
    "#evaluation(test_neg_data, 'neg')\n",
    "correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean = evaluation(test_neg_data, 'neg')\n",
    "print(correct_count)\n",
    "print(lm_scores_mean)\n",
    "print(correct_count)\n",
    "print(lm_scores_mean)\n",
    "print(similarity_scores_mean)\n",
    "print(masked_similarity_scores_mean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
