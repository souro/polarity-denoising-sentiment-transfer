{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d812ba-03a0-49a5-8625-69b993c94f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dill\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "from spacy.symbols import ORTH\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/')\n",
    "import SentimentTransfer_Evaluations as snt_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bf739-011d-4ad5-8a78-2b4eecf8a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e60ad7-ffab-4e18-b7fa-25871007dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "spacy_en.tokenizer.add_special_case(u'<n>', [{ORTH: u'<n>'}])\n",
    "spacy_en.tokenizer.add_special_case(u'<p>', [{ORTH: u'<p>'}])\n",
    "spacy_de = spacy.load('de_core_news_sm')\n",
    "spacy_de.tokenizer.add_special_case(u'<n>', [{ORTH: u'<n>'}])\n",
    "spacy_de.tokenizer.add_special_case(u'<p>', [{ORTH: u'<p>'}])\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbbe27-53ba-44e5-af4e-3c7294bb0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_field(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return dill.load(f)\n",
    "\n",
    "SRC = None\n",
    "TRG = None\n",
    "if os.path.exists('baseline_styletoken/src.field'):\n",
    "    SRC = load_field(os.path.join(\"baseline_styletoken/\", 'src.field'))\n",
    "if os.path.exists('baseline_styletoken/trg.field'):\n",
    "    TRG = load_field(os.path.join(\"baseline_styletoken/\", 'trg.field'))\n",
    "else:\n",
    "    SRC = Field(tokenize = tokenize_fr,\n",
    "                # tokenize = 'spacy',\n",
    "                # tokenizer_language='en',\n",
    "                #init_token='<sos',\n",
    "                eos_token = '<eos>',\n",
    "                lower = True,\n",
    "                batch_first = True,\n",
    "                fix_length=100)\n",
    "\n",
    "    TRG = Field(tokenize = tokenize_en,\n",
    "                # tokenize = 'spacy',\n",
    "                # tokenizer_language='de',\n",
    "                #init_token='<sos>',\n",
    "                eos_token = '<eos>',\n",
    "                lower = True,\n",
    "                batch_first = True,\n",
    "                fix_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccaf900-7f95-4776-8452-140d913f3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de,\n",
    "            # tokenize = 'spacy',\n",
    "            # tokenizer_language='en',\n",
    "            #init_token='<sos',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            batch_first = True,\n",
    "            fix_length=100)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en,\n",
    "            # tokenize = 'spacy',\n",
    "            # tokenizer_language='de',\n",
    "            #init_token='<sos>',\n",
    "            eos_token = '<eos>',\n",
    "            lower = True,\n",
    "            batch_first = True,\n",
    "            fix_length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89218247-39a1-4789-b18f-b2a477b78183",
   "metadata": {},
   "source": [
    "### add '\\<pos\\>' and '\\<neg\\' in the corresponding pos and neg infront of each sentence in the corresponding pos and neg file (in the 'data' directory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a1dab-fdc0-4d0b-8fa9-dea2fb5eff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow the command to do the above: sed -i -e 's/^/<p> /' <file_details>\n",
    "\n",
    "# (Incase if you want to undo the changes, please follow the command: sed -i -e 's/[^ ]* //')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7545ea7a-5bed-44df-83b4-3f38c97aed0a",
   "metadata": {},
   "source": [
    "### Join the above processed pos and neg file (in the 'data' directory) together and mention the path below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effae047-ae72-47fd-8867-02543f98d68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TranslationDataset(\n",
    "    path = \"\",\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "valid_data = TranslationDataset(\n",
    "    path = \"\",\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG),\n",
    ")\n",
    "test_data = TranslationDataset(\n",
    "    path = \"\",\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75363db-5304-4636-811c-e84a8b55ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\", flush=True)\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\", flush=True)\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7be43d-3c2a-4ef4-a2b7-c75fdb6d39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_field(field,name, savedir=\"baseline_styletoken/\"):\n",
    "    save_path = os.path.join(savedir, f\"{name}.field\")\n",
    "    with open(save_path, 'wb') as fout:\n",
    "        dill.dump(field, fout)\n",
    "\n",
    "def save_vocab(field, name, savedir=\"baseline_styletoken/\"):\n",
    "    save_path = os.path.join(savedir, f\"{name}_vocab.txt\")\n",
    "    with open(save_path, 'w') as fout:\n",
    "        for w in field.vocab.itos:\n",
    "            fout.write(w + '\\n')\n",
    "\n",
    "specials=['<p>', '<n>']\n",
    "if os.path.exists('baseline_styletoken/src.field') and os.path.exists('baseline_styletoken/src.vocab'):\n",
    "    pass\n",
    "else:\n",
    "    SRC.build_vocab(train_data, min_freq = 2, max_size=30000, specials=specials)\n",
    "    save_field(SRC, \"src\", \"baseline_styletoken\")\n",
    "    save_vocab(SRC, \"src\", \"baseline_styletoken\")\n",
    "if os.path.exists('baseline_styletoken/trg.field') and os.path.exists('baseline_styletoken/trg.vocab'):\n",
    "    pass\n",
    "else:\n",
    "    TRG.build_vocab(train_data, min_freq = 2, max_size=30000, specials=specials)\n",
    "    save_field(TRG, \"trg\", \"baseline_styletoken\")\n",
    "    save_vocab(TRG, \"trg\", \"baseline_styletoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a5257-60f3-4533-a583-7ac2028c02ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "specials=['<p>', '<n>']\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2, max_size=30000, specials=specials)\n",
    "\n",
    "TRG.build_vocab(train_data, min_freq = 2, max_size=30000, specials=specials)\n",
    "\n",
    "print(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\", flush=True)\n",
    "print(f\"Unique tokens in target (fr) vocabulary: {len(TRG.vocab)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b892c-66f1-4b73-b9c0-76465453f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a90b11-7276-4155-97bd-8fce2d0c4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key= lambda x: len(x.src),\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e014ef-5c10-421a-b429-63a9963c7e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, src, style_embedd_enc, src_mask):\n",
    "        # src = [batch size, src len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, src len]\n",
    "\n",
    "        #src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos) + style_embedd_enc)\n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065d8a9-edae-4fad-b0e1-7cecc9ee1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # src = [batch size, src len, hid dim]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        src = self.layer_norm(src + self.dropout(_src))\n",
    "\n",
    "        # src = [batch size, src len, hid dim]\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747db36a-d48f-4b84-9d99-8c09c1b62ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "\n",
    "        assert hid_dim % n_heads == 0\n",
    "\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.shape[0]\n",
    "\n",
    "        # query = [batch size, query len, hid dim]\n",
    "        # key = [batch size, key len, hid dim]\n",
    "        # value = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "\n",
    "        # Q = [batch size, query len, hid dim]\n",
    "        # K = [batch size, key len, hid dim]\n",
    "        # V = [batch size, value len, hid dim]\n",
    "\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # Q = [batch size, n heads, query len, head dim]\n",
    "        # K = [batch size, n heads, key len, head dim]\n",
    "        # V = [batch size, n heads, value len, head dim]\n",
    "\n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "\n",
    "        # energy = [batch size, n heads, query len, key len]\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        # attention = [batch size, n heads, query len, key len]\n",
    "\n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "\n",
    "        # x = [batch size, n heads, query len, head dim]\n",
    "\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        # x = [batch size, query len, n heads, head dim]\n",
    "\n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        x = self.fc_o(x)\n",
    "\n",
    "        # x = [batch size, query len, hid dim]\n",
    "\n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee5d512-1497-435e-8036-26c51272d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "\n",
    "        # x = [batch size, seq len, pf dim]\n",
    "\n",
    "        x = self.fc_2(x)\n",
    "\n",
    "        # x = [batch size, seq len, hid dim]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22789cfb-8852-44c5-928f-d4e23210b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 hid_dim,\n",
    "                 n_layers,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device,\n",
    "                 max_length=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim,\n",
    "                                                  n_heads,\n",
    "                                                  pf_dim,\n",
    "                                                  dropout,\n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "\n",
    "    def forward(self, trg, enc_src, style_embedd_dec, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "\n",
    "        # pos = [batch size, trg len]\n",
    "\n",
    "        #trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos) + style_embedd_dec)\n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        output = self.fc_out(trg)\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7d6ab-65fe-4f03-9108-9dd1f94267aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hid_dim,\n",
    "                 n_heads,\n",
    "                 pf_dim,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
    "                                                                     pf_dim,\n",
    "                                                                     dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # trg_mask = [batch size, trg len]\n",
    "        # src_mask = [batch size, src len]\n",
    "\n",
    "        # self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "\n",
    "        # dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "\n",
    "        # positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "\n",
    "        # dropout, residual and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "\n",
    "        # trg = [batch size, trg len, hid dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed7e02-7cdc-4890-a0e8-629f872f0680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 src_pad_idx,\n",
    "                 trg_pad_idx,\n",
    "                 device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        self.style_embedding = nn.Embedding(5, 512)\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        # src = [batch size, src len]\n",
    "\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "\n",
    "        # trg_pad_mask = [batch size, 1, trg len, 1]\n",
    "\n",
    "        trg_len = trg.shape[1]\n",
    "\n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\n",
    "\n",
    "        # trg_sub_mask = [trg len, trg len]\n",
    "\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, style_tok_list_tensor_enc, style_tok_list_tensor_dec, trg):\n",
    "        # src = [batch size, src len]\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        # src_mask = [batch size, 1, 1, src len]\n",
    "        # trg_mask = [batch size, 1, trg len, trg len]\n",
    "\n",
    "        style_embedd_enc = self.style_embedding(style_tok_list_tensor_enc)\n",
    "        style_embedd_dec = self.style_embedding(style_tok_list_tensor_dec)\n",
    "\n",
    "        enc_src = self.encoder(src, style_embedd_enc, src_mask)\n",
    "\n",
    "        # enc_src = [batch size, src len, hid dim]\n",
    "        # enc_src1 = style_embedd.add(enc_src_before)\n",
    "        # enc_src = torch.div(enc_src1,2)\n",
    "\n",
    "        # enc_src = torch.cat((enc_src_before, style_embedd), dim=2)\n",
    "\n",
    "        output, attention = self.decoder(trg, enc_src, style_embedd_dec, trg_mask, src_mask)\n",
    "\n",
    "        # output = [batch size, trg len, output dim]\n",
    "        # attention = [batch size, n heads, trg len, src len]\n",
    "\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec395f2-2e22-4c76-b700-9979d4de3846",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "#HID_DIM = 512\n",
    "HID_DIM_ENC = 512\n",
    "HID_DIM_DEC = 512\n",
    "ENC_LAYERS = 4\n",
    "DEC_LAYERS = 4\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM,\n",
    "              HID_DIM_ENC,\n",
    "              ENC_LAYERS,\n",
    "              ENC_HEADS,\n",
    "              ENC_PF_DIM,\n",
    "              ENC_DROPOUT,\n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM,\n",
    "              HID_DIM_DEC,\n",
    "              DEC_LAYERS,\n",
    "              DEC_HEADS,\n",
    "              DEC_PF_DIM,\n",
    "              DEC_DROPOUT,\n",
    "              device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8bccc-a18f-4118-b0b2-dd5d2285161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9209349-eead-4cee-b61a-eeee2ed82551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters', flush=True)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4f628c-27c5-4807-bcca-e334321e2bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daadf9d-3c7d-4837-b3e0-56b8d7c9b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('baseline_styletoken/checkpoint.pt')\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "# save_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a27b552-b025-4a5a-8b45-0b695bd39b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list_enc = [3 for i in range(100)]\n",
    "neg_list_enc = [4 for i in range(100)]\n",
    "\n",
    "pos_list_dec = [3 for i in range(99)]\n",
    "neg_list_dec = [4 for i in range(99)]\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip, epoch):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_style_loss = 0\n",
    "    epoch_translation_loss = 0\n",
    "\n",
    "    loss_bool = True\n",
    "\n",
    "    list_translation_loss = []\n",
    "    list_style_loss = []\n",
    "    loss_counter = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        #torch.set_printoptions(edgeitems=50)\n",
    "        #print(src)\n",
    "        trg = batch.trg\n",
    "\n",
    "        style_tok_list_enc = []\n",
    "        style_tok_list_dec = []\n",
    "        style_tok_list_dec_ops = []\n",
    "        senti_trg_list = []\n",
    "        senti_trg_list_ops = []\n",
    "\n",
    "        # import pudb\n",
    "        # pudb.set_trace()\n",
    "\n",
    "        for i, x in enumerate(src.cpu().numpy()):\n",
    "            # for j, k in enumerate(x):\n",
    "            if (x[0] == 3):\n",
    "                #senti_trg_list_ops.append(0)\n",
    "                senti_trg_list_ops.append(0)\n",
    "                style_tok_list_enc.append(pos_list_enc)\n",
    "                style_tok_list_dec.append(pos_list_dec)\n",
    "                style_tok_list_dec_ops.append(neg_list_dec)\n",
    "            elif (x[0] == 4):\n",
    "                senti_trg_list_ops.append(0)\n",
    "                style_tok_list_enc.append(neg_list_enc)\n",
    "                style_tok_list_dec.append(neg_list_dec)\n",
    "                style_tok_list_dec_ops.append(pos_list_dec)\n",
    "        style_tok_list_np_enc = np.array(style_tok_list_enc)\n",
    "        style_tok_list_np_dec = np.array(style_tok_list_dec)\n",
    "        style_tok_list_np_dec_ops = np.array(style_tok_list_dec_ops)\n",
    "        style_tok_list_tensor_enc = torch.from_numpy(style_tok_list_np_enc).to(device)\n",
    "        style_tok_list_tensor_dec = torch.from_numpy(style_tok_list_np_dec).to(device)\n",
    "        style_tok_list_tensor_dec_ops = torch.from_numpy(style_tok_list_np_dec_ops).to(device)\n",
    "\n",
    "        trg_copy = trg.detach().clone()\n",
    "        trg_ops_list = []\n",
    "        for i, x in enumerate(trg_copy.cpu().numpy()):\n",
    "            # if (x[0]==3):\n",
    "            #     x[0]=4\n",
    "            # elif (x[0]==4):\n",
    "            #     x[0]=3\n",
    "            if (x[0] == 3):\n",
    "                x[0] = 4\n",
    "            trg_ops_list.append(x)\n",
    "            trg_ops_np = np.array(trg_ops_list)\n",
    "            trg_ops = torch.from_numpy(trg_ops_np).to(device)\n",
    "\n",
    "\n",
    "        # import pudb\n",
    "        # pudb.set_trace()\n",
    "\n",
    "        # new_src_list = []\n",
    "        # for i in src.cpu().numpy().tolist():\n",
    "        #     i.pop(1)\n",
    "        #     i.append(1)\n",
    "        #     new_src_list.append(i)\n",
    "        # new_src_ndarray = np.array(new_src_list)\n",
    "        # new_src_tensor = torch.from_numpy(new_src_ndarray).to(device)\n",
    "        # new_trg_list = []\n",
    "        # for i in trg.cpu().numpy().tolist():\n",
    "        #     i.pop(1)\n",
    "        #     i.append(1)\n",
    "        #     new_trg_list.append(i)\n",
    "        # new_trg_ndarray = np.array(new_trg_list)\n",
    "        # new_trg_tensor = torch.from_numpy(new_trg_ndarray).to(device)\n",
    "        # src = new_src_tensor\n",
    "        # trg = new_trg_tensor\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, _ = model(src, style_tok_list_tensor_enc, style_tok_list_tensor_dec, trg[:, :-1])\n",
    "        output1, _ = model(src, style_tok_list_tensor_enc, style_tok_list_tensor_dec_ops, trg_ops[:, :-1])\n",
    "        # output = [batch size, trg len - 1, output dim]\n",
    "        # trg = [batch size, trg len]\n",
    "\n",
    "\n",
    "        #########################################################################################\n",
    "        senti_loss2 = None\n",
    "        if(epoch>=0):\n",
    "            senti_score_list = []\n",
    "            senti_score_list2 = []\n",
    "            for i, x in enumerate(output1.cpu().detach().numpy()):\n",
    "                # single_tensor = torch.from_numpy(x)\n",
    "                gen_trg_indexes = []\n",
    "                for x_ind in x:\n",
    "                    single_tensor = torch.from_numpy(x_ind)\n",
    "                    gen_pred_token = single_tensor.argmax().item()\n",
    "                    gen_trg_indexes.append(gen_pred_token)\n",
    "                    if gen_pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                        break\n",
    "                trg_tokens = [TRG.vocab.itos[i] for i in gen_trg_indexes]\n",
    "                # if(epoch==25):\n",
    "                #     print(post_processing(trg_tokens), flush=True)\n",
    "                #     print('\\n', flush=True)\n",
    "\n",
    "                # import pudb\n",
    "                # pudb.set_trace()\n",
    "                #trn_sent = translate_sentence(post_processing(trg_tokens), SRC, TRG, model, device, is_st=True)\n",
    "                senti_pred_score = snt_ev.senti_score(post_processing(trg_tokens))\n",
    "                senti_score_list.append(senti_pred_score)\n",
    "                senti_score_list2.append([senti_pred_score for i in range(100)])\n",
    "\n",
    "            #senti_pred = np.array(senti_score_list)\n",
    "            senti_trg = np.array(senti_trg_list_ops)\n",
    "            #senti_pred_tensor = torch.tensor(senti_pred, requires_grad=True)\n",
    "            senti_trg_tensor = torch.tensor(senti_trg)\n",
    "            #loss = nn.CrossEntropyLoss()\n",
    "            loss = nn.BCELoss()\n",
    "            #senti_loss = loss(senti_pred_tensor, senti_trg_tensor)\n",
    "\n",
    "            senti_pred2 = np.array(senti_score_list)\n",
    "            senti_pred_tensor2 = torch.tensor(senti_pred2, requires_grad=True)\n",
    "            senti_trg_tensor = senti_trg_tensor.double()\n",
    "            senti_loss2 = loss(senti_pred_tensor2, senti_trg_tensor)\n",
    "        ################################################################################################\n",
    "\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # output = [batch size * trg len - 1, output dim]\n",
    "        # trg = [batch size * trg len - 1]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        new_loss = None\n",
    "        actual_loss = None\n",
    "        if(epoch>=0):\n",
    "            #new_loss = loss + senti_loss2\n",
    "            new_loss = loss\n",
    "            # new_loss = None\n",
    "            # if(loss_bool == True):\n",
    "            #     new_loss = loss\n",
    "            #     loss_bool = False\n",
    "            # else:\n",
    "            #     new_loss = senti_loss2\n",
    "            #     loss_bool = True\n",
    "            # print(\"Loss Testing\", flush=True)\n",
    "            # print(\"######################################################################################\", flush=True)\n",
    "            # print(loss.item(), flush=True)\n",
    "            # print(senti_loss2.item(), flush=True)\n",
    "            # print(new_loss.item(), flush=True)\n",
    "            # print(\"######################################################################################\", flush=True)\n",
    "            # print(\"\\n\")\n",
    "            new_loss.backward()\n",
    "            actual_loss = new_loss\n",
    "        else:\n",
    "            loss.backward()\n",
    "            actual_loss = loss\n",
    "\n",
    "        list_translation_loss.append(loss.item())\n",
    "        list_style_loss.append(senti_loss2.item())\n",
    "        #loss_counter += 1\n",
    "        # if (loss_counter % 1000 == 0):\n",
    "        #     print(\n",
    "        #         f'Translation Loss: {sum(list_translation_loss) / len(list_translation_loss)},Style Loss: {sum(list_style_loss) / len(list_style_loss)}',\n",
    "        #         flush=True)\n",
    "        #     list_translation_loss.clear()\n",
    "        #     list_style_loss.clear()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_translation_loss += loss.item()\n",
    "        epoch_style_loss += senti_loss2.item()\n",
    "        epoch_loss += actual_loss.item()\n",
    "\n",
    "    epoch_translation_loss_avg = epoch_translation_loss / len(iterator)\n",
    "    epoch_style_loss_avg = epoch_style_loss / len(iterator)\n",
    "    epoch_overall_loss = epoch_loss / len(iterator)\n",
    "\n",
    "    return epoch_translation_loss_avg, epoch_style_loss_avg, epoch_overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7517adf0-47d1-4ef9-a4ab-e2a31bb010f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "def post_processing(text_list):\n",
    "    repl_list = {'@@ ': '', '<eos>':''}\n",
    "    text_str = ' '.join(text_list)\n",
    "    return functools.reduce(lambda a, kv: a.replace(*kv), repl_list.items(), text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e436912-657e-4015-9268-f57e2e834fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    #####################\n",
    "    epoch_style_loss = 0\n",
    "    epoch_translation_loss = 0\n",
    "    loss_bool = True\n",
    "    list_translation_loss = []\n",
    "    list_style_loss = []\n",
    "    loss_counter = 0\n",
    "    #####################\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            ############################\n",
    "            style_tok_list_enc = []\n",
    "            style_tok_list_dec = []\n",
    "            style_tok_list_dec_ops = []\n",
    "            senti_trg_list = []\n",
    "            senti_trg_list_ops = []\n",
    "            ############################\n",
    "\n",
    "            ####################################\n",
    "            for i, x in enumerate(src.cpu().numpy()):\n",
    "                # for j, k in enumerate(x):\n",
    "                if (x[0] == 3):\n",
    "                    senti_trg_list_ops.append(0)\n",
    "                    style_tok_list_enc.append(pos_list_enc)\n",
    "                    style_tok_list_dec.append(pos_list_dec)\n",
    "                    style_tok_list_dec_ops.append(neg_list_dec)\n",
    "                elif (x[0] == 4):\n",
    "                    senti_trg_list_ops.append(1)\n",
    "                    style_tok_list_enc.append(neg_list_enc)\n",
    "                    style_tok_list_dec.append(neg_list_dec)\n",
    "                    style_tok_list_dec_ops.append(pos_list_dec)\n",
    "            style_tok_list_np_enc = np.array(style_tok_list_enc)\n",
    "            style_tok_list_np_dec = np.array(style_tok_list_dec)\n",
    "            style_tok_list_np_dec_ops = np.array(style_tok_list_dec_ops)\n",
    "            style_tok_list_tensor_enc = torch.from_numpy(style_tok_list_np_enc).to(device)\n",
    "            style_tok_list_tensor_dec = torch.from_numpy(style_tok_list_np_dec).to(device)\n",
    "            style_tok_list_tensor_dec_ops = torch.from_numpy(style_tok_list_np_dec_ops).to(device)\n",
    "\n",
    "            trg_copy = trg.detach().clone()\n",
    "            trg_ops_list = []\n",
    "            for i, x in enumerate(trg_copy.cpu().numpy()):\n",
    "                if (x[0] == 3):\n",
    "                    x[0] = 4\n",
    "                elif (x[0] == 4):\n",
    "                    x[0] = 3\n",
    "                trg_ops_list.append(x)\n",
    "                trg_ops_np = np.array(trg_ops_list)\n",
    "                trg_ops = torch.from_numpy(trg_ops_np).to(device)\n",
    "            ##########################################\n",
    "\n",
    "            # new_src_list = []\n",
    "            # for i in src.cpu().numpy().tolist():\n",
    "            #     i.pop(1)\n",
    "            #     i.append(1)\n",
    "            #     new_src_list.append(i)\n",
    "            # new_src_ndarray = np.array(new_src_list)\n",
    "            # new_src_tensor = torch.from_numpy(new_src_ndarray).to(device)\n",
    "            # new_trg_list = []\n",
    "            # for i in trg.cpu().numpy().tolist():\n",
    "            #     i.pop(1)\n",
    "            #     i.append(1)\n",
    "            #     new_trg_list.append(i)\n",
    "            # new_trg_ndarray = np.array(new_trg_list)\n",
    "            # new_trg_tensor = torch.from_numpy(new_trg_ndarray).to(device)\n",
    "            # src = new_src_tensor\n",
    "            # trg = new_trg_tensor\n",
    "\n",
    "            #output, _ = model(src, style_tok_list_tensor_enc, style_tok_list_tensor_dec, trg[:, :-1])\n",
    "\n",
    "            #######################################################\n",
    "            output, _ = model(src, style_tok_list_tensor_enc, style_tok_list_tensor_dec, trg[:, :-1])\n",
    "            output1, _ = model(src, style_tok_list_tensor_enc, style_tok_list_tensor_dec_ops, trg_ops[:, :-1])\n",
    "            #######################################################\n",
    "\n",
    "            # output = [batch size, trg len - 1, output dim]\n",
    "            # trg = [batch size, trg len]\n",
    "\n",
    "            #########################################################################################\n",
    "            senti_loss2 = None\n",
    "            if (epoch >= 0):\n",
    "                senti_score_list = []\n",
    "                senti_score_list2 = []\n",
    "                for i, x in enumerate(output1.cpu().detach().numpy()):\n",
    "                    # single_tensor = torch.from_numpy(x)\n",
    "                    gen_trg_indexes = []\n",
    "                    for x_ind in x:\n",
    "                        single_tensor = torch.from_numpy(x_ind)\n",
    "                        gen_pred_token = single_tensor.argmax().item()\n",
    "                        gen_trg_indexes.append(gen_pred_token)\n",
    "                        if gen_pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                            break\n",
    "                    trg_tokens = [TRG.vocab.itos[i] for i in gen_trg_indexes]\n",
    "                    # if(epoch==25):\n",
    "                    #     print(post_processing(trg_tokens), flush=True)\n",
    "                    #     print('\\n', flush=True)\n",
    "\n",
    "                    # import pudb\n",
    "                    # pudb.set_trace()\n",
    "                    # trn_sent = translate_sentence(post_processing(trg_tokens), SRC, TRG, model, device, is_st=True)\n",
    "                    senti_pred_score = snt_ev.senti_score(post_processing(trg_tokens))\n",
    "                    senti_score_list.append(senti_pred_score)\n",
    "                    senti_score_list2.append([senti_pred_score for i in range(100)])\n",
    "\n",
    "                # senti_pred = np.array(senti_score_list)\n",
    "                senti_trg = np.array(senti_trg_list_ops)\n",
    "                # senti_pred_tensor = torch.tensor(senti_pred, requires_grad=True)\n",
    "                senti_trg_tensor = torch.tensor(senti_trg)\n",
    "                # loss = nn.CrossEntropyLoss()\n",
    "                loss = nn.BCELoss()\n",
    "                # senti_loss = loss(senti_pred_tensor, senti_trg_tensor)\n",
    "\n",
    "                senti_pred2 = np.array(senti_score_list)\n",
    "                senti_pred_tensor2 = torch.tensor(senti_pred2, requires_grad=True)\n",
    "                senti_trg_tensor = senti_trg_tensor.double()\n",
    "                senti_loss2 = loss(senti_pred_tensor2, senti_trg_tensor)\n",
    "            ################################################################################################\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            # output = [batch size * trg len - 1, output dim]\n",
    "            # trg = [batch size * trg len - 1]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            new_loss = None\n",
    "            actual_loss = None\n",
    "            if (epoch >= 0):\n",
    "                #new_loss = loss + senti_loss2\n",
    "                new_loss = loss\n",
    "                # new_loss = None\n",
    "                # if(loss_bool == True):\n",
    "                #     new_loss = loss\n",
    "                #     loss_bool = False\n",
    "                # else:\n",
    "                #     new_loss = senti_loss2\n",
    "                #     loss_bool = True\n",
    "                # print(\"Loss Testing\", flush=True)\n",
    "                # print(\"######################################################################################\", flush=True)\n",
    "                # print(loss.item(), flush=True)\n",
    "                # print(senti_loss2.item(), flush=True)\n",
    "                # print(new_loss.item(), flush=True)\n",
    "                # print(\"######################################################################################\", flush=True)\n",
    "                # print(\"\\n\")\n",
    "                actual_loss = new_loss\n",
    "            else:\n",
    "                actual_loss = loss\n",
    "\n",
    "            list_translation_loss.append(loss.item())\n",
    "            list_style_loss.append(senti_loss2.item())\n",
    "            # loss_counter += 1\n",
    "            # if (loss_counter % 1000 == 0):\n",
    "            #     print(\n",
    "            #         f'Translation Loss: {sum(list_translation_loss) / len(list_translation_loss)},Style Loss: {sum(list_style_loss) / len(list_style_loss)}',\n",
    "            #         flush=True)\n",
    "            #     list_translation_loss.clear()\n",
    "            #     list_style_loss.clear()\n",
    "\n",
    "            epoch_translation_loss += loss.item()\n",
    "            epoch_style_loss += senti_loss2.item()\n",
    "            epoch_loss += actual_loss.item()\n",
    "\n",
    "        epoch_translation_loss_avg = epoch_translation_loss / len(iterator)\n",
    "        epoch_style_loss_avg = epoch_style_loss / len(iterator)\n",
    "        epoch_overall_loss = epoch_loss / len(iterator)\n",
    "\n",
    "        return epoch_translation_loss_avg, epoch_style_loss_avg, epoch_overall_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4f4748-373c-4e24-a6b7-c6ac743a1cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170a51a-74c5-43fd-a3d5-12e6d9357437",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "valid_loss_before = float('inf')\n",
    "\n",
    "early_stop_cnt = 0\n",
    "early_stop_lookout = 5\n",
    "early_stop=False\n",
    "\n",
    "another_early_stop_cnt = 0\n",
    "another_early_stop_lookout = 10\n",
    "another_early_stop=False\n",
    "\n",
    "best_epoch_no = 1\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    epoch_translation_loss_avg, epoch_style_loss_avg, epoch_overall_loss = train(model, train_iterator, optimizer, criterion, CLIP, epoch)\n",
    "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    valid_translation_loss_avg, valid_style_loss_avg, valid_overall_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    #Early_Stop\n",
    "    if valid_overall_loss < best_valid_loss or valid_overall_loss < valid_loss_before:\n",
    "        early_stop_cnt = 0\n",
    "        early_stop = False\n",
    "\n",
    "    elif valid_overall_loss >= best_valid_loss or valid_overall_loss >= valid_loss_before:\n",
    "        early_stop_cnt += 1\n",
    "        early_stop = True\n",
    "\n",
    "    # Another Early_Stop based on only best valid loss\n",
    "    if epoch >= 20 :\n",
    "        if valid_overall_loss < best_valid_loss:\n",
    "            another_early_stop_cnt = 0\n",
    "            another_early_stop = False\n",
    "\n",
    "        elif valid_overall_loss >= best_valid_loss:\n",
    "            another_early_stop_cnt += 1\n",
    "            another_early_stop = True\n",
    "\n",
    "\n",
    "    if valid_overall_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_overall_loss\n",
    "        best_epoch_no = epoch\n",
    "\n",
    "        torch.save(checkpoint, 'baseline_styletoken/checkpoint.pt')\n",
    "        # if(epoch==4):\n",
    "        #     torch.save(checkpoint, 'baseline_styletoken/epoch5_checkpoint.pt')\n",
    "\n",
    "    valid_loss_before = valid_overall_loss\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s', flush=True)\n",
    "    print(f'\\tTrain Translation Loss: {epoch_translation_loss_avg:.3f}', flush=True)\n",
    "    print(f'\\tTrain Style Loss: {epoch_style_loss_avg:.3f}', flush=True)\n",
    "    print(f'\\tTrain Overall Loss: {epoch_overall_loss:.3f} | Train PPL: {math.exp(epoch_overall_loss):7.3f}', flush=True)\n",
    "\n",
    "    print(f'\\t Val. Translation Loss: {valid_translation_loss_avg:.3f}', flush=True)\n",
    "    print(f'\\t Val. Style Loss: {valid_style_loss_avg:.3f}', flush=True)\n",
    "    print(f'\\t Val. Overall Loss: {valid_overall_loss:.3f} |  Val. PPL: {math.exp(valid_overall_loss):7.3f}', flush=True)\n",
    "    print(f'\\t Till now Best Val. Loss: {best_valid_loss:.3f} found on {best_epoch_no+1} epoch ', flush=True)\n",
    "\n",
    "    if early_stop==True :\n",
    "        print(f'EarlyStopping counter (1st way): {early_stop_cnt} out of {early_stop_lookout}', flush=True)\n",
    "\n",
    "    if another_early_stop==True :\n",
    "        print(f'EarlyStopping counter (2nd way): {another_early_stop_cnt} out of {another_early_stop_lookout}', flush=True)\n",
    "\n",
    "    print('\\n', flush=True)\n",
    "\n",
    "\n",
    "    if early_stop_cnt == early_stop_lookout:\n",
    "        print('Early Stoping (1st way)...', flush=True)\n",
    "        break\n",
    "    if another_early_stop_cnt == another_early_stop_lookout:\n",
    "        print('Early Stoping (2nd way)...', flush=True)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e16c41-e944-41d8-82fe-0928c1388699",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('baseline_styletoken/checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "#epoch = checkpoint['epoch']\n",
    "\n",
    "#test_loss = evaluate(model, test_iterator, criterion)\n",
    "#\n",
    "# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e082a5-5290-4b57-833f-26d5c5978e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, src_field, trg_field, model, device, max_len=50, is_st=False):\n",
    "    model.eval()\n",
    "\n",
    "    if isinstance(sentence, str):\n",
    "        nlp = spacy.load('en')\n",
    "        tokens = [token.text.lower() for token in nlp(sentence)]\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "\n",
    "    #tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    tokens = tokens + [src_field.eos_token]\n",
    "\n",
    "    src_indexes = [src_field.vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "    src_mask = model.make_src_mask(src_tensor)\n",
    "\n",
    "    ######################################################\n",
    "    pos_list_enc = [3 for i in range(src_tensor.shape[1])]\n",
    "    neg_list_enc = [4 for i in range(src_tensor.shape[1])]\n",
    "\n",
    "    # pos_list_dec = [3 for i in range(src_tensor.shape[1]-1)]\n",
    "    # neg_list_dec = [4 for i in range(src_tensor.shape[1]-1)]\n",
    "\n",
    "    style_tok_list_enc = []\n",
    "    if (src_field.vocab.stoi[tokens[0]] == 3):\n",
    "        style_tok_list_enc.append(pos_list_enc)\n",
    "    elif (src_field.vocab.stoi[tokens[0]] == 4):\n",
    "        style_tok_list_enc.append(neg_list_enc)\n",
    "    style_tok_list_np_enc = np.array(style_tok_list_enc)\n",
    "    style_tok_list_tensor_enc = torch.from_numpy(style_tok_list_np_enc).to(device)\n",
    "    style_embedd_enc = model.style_embedding(style_tok_list_tensor_enc)\n",
    "    ##############################################################################\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_src = model.encoder(src_tensor, style_embedd_enc, src_mask)\n",
    "\n",
    "    trg_indexes = None\n",
    "    if(is_st == False):\n",
    "        trg_indexes = [trg_field.vocab.stoi[tokens[0]]]\n",
    "    else:\n",
    "        if tokens[0] == '<p>':\n",
    "            trg_indexes = [trg_field.vocab.stoi['<n>']]\n",
    "        elif tokens[0] == '<n>':\n",
    "            trg_indexes = [trg_field.vocab.stoi['<p>']]\n",
    "\n",
    "    if (trg_field.vocab.stoi[tokens[0]] == 3):\n",
    "        if (is_st == True):\n",
    "            senti_trg_list_ops.append(0)\n",
    "    elif (trg_field.vocab.stoi[tokens[0]] == 4):\n",
    "        if (is_st == True):\n",
    "            senti_trg_list_ops.append(1)\n",
    "\n",
    "    for i in range(max_len):\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "\n",
    "        trg_mask = model.make_trg_mask(trg_tensor)\n",
    "\n",
    "        #########################################################\n",
    "        pos_list_dec = [3 for i in range(trg_tensor.shape[1])]\n",
    "        neg_list_dec = [4 for i in range(trg_tensor.shape[1])]\n",
    "        style_tok_list_dec = []\n",
    "        if (trg_field.vocab.stoi[tokens[0]] == 3):\n",
    "            if (is_st == False):\n",
    "                style_tok_list_dec.append(pos_list_dec)\n",
    "            else:\n",
    "                style_tok_list_dec.append(neg_list_dec)\n",
    "        elif (trg_field.vocab.stoi[tokens[0]] == 4):\n",
    "            if (is_st == False):\n",
    "                style_tok_list_dec.append(neg_list_dec)\n",
    "            else:\n",
    "                style_tok_list_dec.append(pos_list_dec)\n",
    "        style_tok_list_np_dec = np.array(style_tok_list_dec)\n",
    "        style_tok_list_tensor_dec = torch.from_numpy(style_tok_list_np_dec).to(device)\n",
    "        style_embedd_dec = model.style_embedding(style_tok_list_tensor_dec)\n",
    "        #########################################################################################\n",
    "        with torch.no_grad():\n",
    "            output, attention = model.decoder(trg_tensor, enc_src, style_embedd_dec, trg_mask, src_mask)\n",
    "\n",
    "        pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "    return trg_tokens[1:], attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66060cd3-cf72-491a-8cb9-405f554f99fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "import nltk\n",
    "senti_trg_list_ops = []\n",
    "def masked_sent(sent_list):\n",
    "    masked = snt_ev.mask_polarity(post_processing(sent_list))\n",
    "    return nltk.word_tokenize(masked)\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len=50, is_st=False):\n",
    "    trgs = []\n",
    "    masked_trgs = []\n",
    "\n",
    "    pred_trgs = []\n",
    "    masked_pred_trgs = []\n",
    "    lengthy_idx = []\n",
    "    for idx, datum in enumerate(data):\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "        if len(src) < 100 and len(trg) < 100:\n",
    "            pred_trg, _ = translate_sentence(src, src_field, trg_field, model, device, max_len, is_st)\n",
    "\n",
    "            # cut off <eos> token\n",
    "            pred_trg = pred_trg[:-1]\n",
    "\n",
    "            pred_trgs.append(pred_trg)\n",
    "            masked_pred_trgs.append(masked_sent(pred_trg))\n",
    "            trgs.append([trg])\n",
    "            masked_trgs.append([masked_sent(trg)])\n",
    "        else:\n",
    "            lengthy_idx.append(idx)\n",
    "\n",
    "    print(lengthy_idx)\n",
    "    print(len(pred_trgs))\n",
    "    return bleu_score(pred_trgs, trgs), bleu_score(masked_pred_trgs, masked_trgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838e8dc-ba8a-4d24-9bfe-bbaa2fd8aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "# print('\\n')\n",
    "# print(f'BLEU score without style transfer = {b_score*100:.2f}', flush=True)\n",
    "\n",
    "b_score, masked_b_score = calculate_bleu(test_data, SRC, TRG, model, device, is_st=True)\n",
    "print('\\n')\n",
    "print(f'BLEU score after style transfer = {b_score*100:.5f}', flush=True)\n",
    "print(f'BLEU score after style transfer = {masked_b_score*100:.5f}', flush=True)\n",
    "\n",
    "\n",
    "def evaluation(data):\n",
    "    correct_count = 0\n",
    "    lm_scores = []\n",
    "    similarity_scores = []\n",
    "    masked_similarity_scores = []\n",
    "\n",
    "    for idx in range(1000):\n",
    "        example_idx = idx\n",
    "\n",
    "        src = vars(data.examples[example_idx])['src']\n",
    "        trg = vars(data.examples[example_idx])['trg']\n",
    "\n",
    "\n",
    "        predicted_trg_st, attention = translate_sentence(src, SRC, TRG, model, device, is_st=True)\n",
    "\n",
    "        print(f'src = {post_processing(src)}', flush=True)\n",
    "\n",
    "        print(f'trg = {post_processing(trg)}', flush=True)\n",
    "        result_trg = snt_ev.senti_score(post_processing(trg))\n",
    "        print(\"Label:\", result_trg['label'])\n",
    "        print(\"Confidence Score:\", result_trg['score'])\n",
    "\n",
    "\n",
    "        print(f'style transfered predicted trg = {post_processing(predicted_trg_st)}', flush=True)\n",
    "\n",
    "        # Sentiment Score\n",
    "        result_pred = snt_ev.senti_score(post_processing(predicted_trg_st))\n",
    "        print(\"Label:\", result_pred['label'])\n",
    "        print(\"Confidence Score:\", result_pred['score'])\n",
    "\n",
    "        if (result_trg['label'] != result_pred['label']):\n",
    "            correct_count += 1\n",
    "\n",
    "        #LM Score\n",
    "        gpt_lm_score = snt_ev.lm_score(post_processing(predicted_trg_st))\n",
    "        print(\"LM Score:\", gpt_lm_score)\n",
    "        lm_scores.append(gpt_lm_score)\n",
    "\n",
    "        #Similarity Score\n",
    "        similarity_score = snt_ev.similarity(post_processing(trg), post_processing(predicted_trg_st))\n",
    "        print('Similarity Score: ', similarity_score)\n",
    "        similarity_scores.append(similarity_score)\n",
    "        similarity_score_masked = snt_ev.similarity(snt_ev.mask_polarity(post_processing(trg)), snt_ev.mask_polarity(post_processing(predicted_trg_st)))\n",
    "        print('Masked Similarity Score: ', similarity_score_masked)\n",
    "        masked_similarity_scores.append(similarity_score_masked)\n",
    "        ###\n",
    "\n",
    "        print('\\n', flush=True)\n",
    "\n",
    "    lm_scores_mean = sum(lm_scores) / len(lm_scores)\n",
    "    similarity_scores_mean = sum(similarity_scores) / len(similarity_scores)\n",
    "    masked_similarity_scores_mean = sum(masked_similarity_scores) / len(masked_similarity_scores)\n",
    "\n",
    "\n",
    "    return correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean\n",
    "\n",
    "\n",
    "print(\"Testing data\", flush=True)\n",
    "print('##############################################################################################################')\n",
    "correct_count, lm_scores_mean, similarity_scores_mean, masked_similarity_scores_mean = evaluation(test_data)\n",
    "print(correct_count)\n",
    "print(lm_scores_mean)\n",
    "print(correct_count)\n",
    "print(lm_scores_mean)\n",
    "print(similarity_scores_mean)\n",
    "print(masked_similarity_scores_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b6c2eb-4ff3-4583-8f21-b8adefc5e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Style Transfer on Training data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# for idx in range(50):\n",
    "#     example_idx = idx\n",
    "#\n",
    "#     src = vars(train_data.examples[example_idx])['src']\n",
    "#     trg = vars(train_data.examples[example_idx])['trg']\n",
    "#\n",
    "#     predicted_trg, attention = translate_sentence(src, SRC, TRG, model, device, is_st=True)\n",
    "#\n",
    "#     print(f'src = {post_processing(src)}', flush=True)\n",
    "#     print(f'trg = {post_processing(trg)}', flush=True)\n",
    "#     print(f'predicted trg = {post_processing(predicted_trg)}', flush=True)\n",
    "#     print('\\n', flush=True)\n",
    "# print('##############################################################################################################')\n",
    "# for idx in range(len(train_data.examples)-50, len(train_data.examples)):\n",
    "#     example_idx = idx\n",
    "#\n",
    "#     src = vars(train_data.examples[example_idx])['src']\n",
    "#     trg = vars(train_data.examples[example_idx])['trg']\n",
    "#\n",
    "#     predicted_trg, attention = translate_sentence(src, SRC, TRG, model, device, is_st=True)\n",
    "#\n",
    "#     print(f'src = {post_processing(src)}', flush=True)\n",
    "#     print(f'trg = {post_processing(trg)}', flush=True)\n",
    "#     print(f'predicted trg = {post_processing(predicted_trg)}', flush=True)\n",
    "#     print('\\n', flush=True)\n",
    "# ######################################################################################################################\n",
    "# print(\"Style Transfer on Testing data\", flush=True)\n",
    "# print('##############################################################################################################')\n",
    "#\n",
    "# senti_score_list = []\n",
    "# # senti_trg_list = []\n",
    "# senti_trg_list_ops = []\n",
    "# # for i, x in enumerate(src.cpu().numpy()):\n",
    "# #     # for j, k in enumerate(x):\n",
    "# #     if (x[0] == 3):\n",
    "# #         senti_trg_list.append(0)\n",
    "# #         senti_trg_list_ops.append(1)\n",
    "# #     elif (x[0] == 4):\n",
    "# #         senti_trg_list.append(1)\n",
    "# #         senti_trg_list_ops.append(0)\n",
    "#\n",
    "# for idx in range(len(test_data.examples)):\n",
    "#     example_idx = idx\n",
    "#\n",
    "#     src = vars(test_data.examples[example_idx])['src']\n",
    "#     trg = vars(test_data.examples[example_idx])['trg']\n",
    "#\n",
    "#     predicted_trg, attention = translate_sentence(src, SRC, TRG, model, device, is_st=True)\n",
    "#\n",
    "#     print(f'src = {post_processing(src)}', flush=True)\n",
    "#     print(f'trg = {post_processing(trg)}', flush=True)\n",
    "#     print(f'predicted trg = {post_processing(predicted_trg)}', flush=True)\n",
    "#     senti_pred_score = snt_ev.senti_score(post_processing(predicted_trg))\n",
    "#     # import pudb\n",
    "#     # pudb.set_trace()\n",
    "#\n",
    "#     senti_score_list.append(senti_pred_score)\n",
    "#     print('\\n', flush=True)\n",
    "#\n",
    "# loss = nn.BCELoss()\n",
    "#\n",
    "# senti_trg = np.array(senti_trg_list_ops)\n",
    "# senti_trg_tensor = torch.tensor(senti_trg)\n",
    "#\n",
    "# senti_pred2 = np.array(senti_score_list)\n",
    "# senti_pred_tensor2 = torch.tensor(senti_pred2, requires_grad=True)\n",
    "# senti_trg_tensor = senti_trg_tensor.double()\n",
    "# senti_loss2 = loss(senti_pred_tensor2, senti_trg_tensor)\n",
    "# print(f'Style Loss: {senti_loss2}', flush=True)\n",
    "# ###########################################################################\n",
    "# def binary_accuracy(preds, y):\n",
    "#     rounded_preds = torch.round(preds)\n",
    "#     correct = (rounded_preds == y).float()\n",
    "#     acc = correct.sum() / len(correct)\n",
    "#     return acc\n",
    "#\n",
    "# # import pudb\n",
    "# # pudb.set_trace()\n",
    "#\n",
    "# style_acc = binary_accuracy(senti_pred_tensor2, senti_trg_tensor)\n",
    "# print(f'Style Accuracy: {style_acc}', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
